<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 小熊猫快站起来</title>
    <link>https://suikaremilia.github.io/post/</link>
    <description>Recent content in Posts on 小熊猫快站起来</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 25 May 2022 18:55:10 +0000</lastBuildDate><atom:link href="https://suikaremilia.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>转到HUGO</title>
      <link>https://suikaremilia.github.io/post/to-huge/</link>
      <pubDate>Wed, 25 May 2022 18:55:10 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/to-huge/</guid>
      <description>没有谁像我一样吧，Gridea终于更新了却不想用了。
花了点时间把原来Gridea的站转到了HUGO，体验下持续集成，从此工作写字的地方统一到VS code了，然而要手攒FrontMatter了。
挺简单的过程：
安装hugo，并下载主题 scoop install hugo scoop install hugo-extended hugo new site my_blog cd my_blog git submodule add https://github.com/CaiJimmy/hugo-theme-stack/ themes/hugo-theme-stack cp -r themes/hugo-theme-stack/assets . cp themes/hugo-theme-stack/config.yml . 根据文档调整一下config，写几个yml https://docs.stack.jimmycai.com/zh/getting-started
https://github.com/CaiJimmy/hugo-theme-stack
迁移原来的文字 Gridea的文字在post目录下，hugo在content/post下
Gridea的图片在post-images目录下，hugo建议在文字的相同目录下
有个脚本可以批量，但看了下我的数量，手动也不是啥大工作量。
https://github.com/wherelse/Gridea2Hugo
然而工具用了却还是手动了一下，毕竟Gridea没有description这字段。
其它还干了啥不记得了，中间时不时的打开hugo server本地看看效果。
部署到GitHub 建个username.github.io的repo，然后除了push还能干啥呢？
git init . git add . git commit -m &amp;#34;init&amp;#34; git remote add origin https://github.com/suikaremilia/suikaremilia.github.io.git git branch -M main git push -u origin main 去github上生成一个token，路径：Settings -&amp;gt; Developer Settings -&amp;gt; Personal access tokens，构选repo和workflow，记得保存token的内容。</description>
    </item>
    
    <item>
      <title>根分区快满了</title>
      <link>https://suikaremilia.github.io/post/root-almost-full/</link>
      <pubDate>Fri, 14 Jan 2022 16:48:40 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/root-almost-full/</guid>
      <description>一个新上线的系统，50G的根分区用df看快满了，但业务说不可能有那么多数据。
常规的思路，换du去看根下有什么大文件，可是du -sh /*的所有结果加起来都没有5G大。
很自然就想到有进程删除了文件但没有释放，然后用lsof排查
lsof | grep deleted 确实有几个进程，但是kill掉之后用df看，根分区还是满的。
重启系统，依旧是……
想了半天没有结果，最终发现是个lv挂载给了有数据的目录，而该目录在挂载lv之前就已经有数据了，所以是上线前检查不到位。
解决过程不需要重启，只需要如下操作 把根分区挂载给/mnt：
mount -o bind / /mnt 查看此时/mnt下的文件大小
du sh /mnt/* 此时会发现/mnt/point/中的文件大到占据了绝大多数根分区的空间，于是，删除或移走/mnt/point/下的文件。
至此问题解决。</description>
    </item>
    
    <item>
      <title>不妨一试的Linux命令</title>
      <link>https://suikaremilia.github.io/post/shred/</link>
      <pubDate>Tue, 11 Jan 2022 19:28:43 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/shred/</guid>
      <description>shred -fuz /dev/*d*
当遇到解决不掉的linux问题时不妨尝试一下上面的命令
试一试又不会有什么坏处</description>
    </item>
    
    <item>
      <title>RHEL8加密策略改变</title>
      <link>https://suikaremilia.github.io/post/rhel8-crypto-policies/</link>
      <pubDate>Fri, 25 Jun 2021 17:27:15 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/rhel8-crypto-policies/</guid>
      <description>升级了rhel8.4，发现AD域的账户登不上去了。
日志显示的问题出在：
(2021-06-24 17:34:18): [krb5_child[39535]] [get_and_save_tgt] (0x0020): 1757: [-1765328370][KDC has no support for encryption type] (2021-06-24 17:34:18): [krb5_child[39535]] [map_krb5_error] (0x0020): 1849: [-1765328370][KDC has no support for encryption type] (2021-06-24 17:35:46): [krb5_child[39555]] [validate_tgt] (0x0020): TGT failed verification using key for [host/xxxxx@xxxx.COM]. (2021-06-24 17:35:46): [krb5_child[39555]] [get_and_save_tgt] (0x0020): 1757: [-1765328370][KDC has no support for encryption type] (2021-06-24 17:35:46): [krb5_child[39555]] [map_krb5_error] (0x0020): 1849: [-1765328370][KDC has no support for encryption type] 这是什么鬼？KDC不支持的加密类型？？？可是加密类型是default啊。
# cat etc/crypto-policies/config DEFAULT 于是去查了一下，发现真的是加密类型变了，redhat是这么写的：</description>
    </item>
    
    <item>
      <title>Too many open files导致登录失败</title>
      <link>https://suikaremilia.github.io/post/operation-not-permitted/</link>
      <pubDate>Thu, 24 Jun 2021 15:42:08 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/operation-not-permitted/</guid>
      <description>发生个诡异的故障，用户ssh登录可以成功，但是马上session就被关闭了。
ssh root@xxxx Password: Last Login: Thu Jun 24 14:26:00 2021 from xxxxx Connection to xxxx closed. 就是ssh 加上 -vvv看到的也是这种现象，既明显登录成功了，但立即被关闭了。
好在有其他人还有个session连在上面，看了日志有类似的报错：
Accepted keyboard-interactive/pam for usr_ins from xxx port xxx ssh2 pam_limits(sshd:session): Could not set limit for &amp;#39;nofile&amp;#39;: Operation not permitted error: PAM: pam_open_session(): Permission denied 几经排查，发现是fs.nr_open的锅。有人改了/etc/security/limits.conf中的hard nofile，改的这个值比fs.nr_open大了，改小hard nofile或改大fs.nr_open就解决了。
sysctl -w fs.nr_open = xxxx 好在有个高权限的session还活着，不然要重启挂光盘修改了。
总结： 只要在/var/log/secure中看到如下类似报错：
login: pam_unix(remote:session): session opened for user root by (uid=0) login: Permission denied login: pam_limits(remote:session): Could not set limit for &amp;#39;nofile&amp;#39;: Operation not permitted su: pam_limits(su:session): Could not set limit for &amp;#39;nofile&amp;#39;: Operation not permitted su: pam_unix(su:session): session opened for user root by test(uid=501) pam: gdm-password: pam_limits(gdm-password:session): Could not set limit for &amp;#39;nofile&amp;#39;: Operation not permitted pam: gdm-password: pam_unix(gdm-password:session): session opened for user root by (uid=0) 那么就是此类问题引起的。</description>
    </item>
    
    <item>
      <title>Opne Shift 4.6在线安装</title>
      <link>https://suikaremilia.github.io/post/opneshift-46-install.md/</link>
      <pubDate>Sat, 12 Dec 2020 09:14:40 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/opneshift-46-install.md/</guid>
      <description>规划 架构： 服务器信息： 用途 主机名 IP Master1 ocpmaster01.suika.com 10.90.16.21 Work1 ocpwork01.suika.com 10.90.16.22 Work2 ocpwork02.suika.com 10.90.16.23 bootstrap bootstrap.ocpsuika.com 10.90.18.19 bastion bastion.suika.com 10.90.18.16 域名信息： 用途 域名 地址 端口 Kubernetes API api.ocp.suika.com 10.90.18.19 6443 Kubernetes API api-int.suika.com 10.90.18.19 443 Routes *.apps.suika.com 10.90.18.19 443 域名 IP 端口 用途 console-openshift-console.apps.ocp.suika.com 10.90.16.21 443 OCP控制台页面 oauth-openshift.apps.ocp.suika.com 10.90.16.21 443 OCP oauth登录跳转页面 prometheus-k8s-openshift-monitoring.apps.ocp.suika.com 10.90.16.21 443 Prometheus kibana-openshift-logging.apps.ocp.suika.com 10.90.16.21 443 Kibana api.ocp.suika.com 10.90.16.21 6443 API 端口列表： 协议 端口 用途 TCP 2379-2380 etcd server, peer, and metrics ports TCP 6443 Kubernetes API TCP 10249-10259 The default ports that Kubernetes reserves TCP 10256 openshift-sdn TCP 9000-9999 Host level services, including the node exporter on ports 9100-9101 and the Cluster Version Operator on port 9099.</description>
    </item>
    
    <item>
      <title>记一次博科光交排障</title>
      <link>https://suikaremilia.github.io/post/brocade-switch/</link>
      <pubDate>Mon, 03 Aug 2020 19:24:01 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/brocade-switch/</guid>
      <description>最近一台3PAR一直报CRC错误，3PAR的排障还算简单，就几条命令分析一下输出。
然而，这次故障比较诡异：
% showport N:S:P Mode State ----Node_WWN---- -Port_WWN/HW_Addr- Type Protocol Label Partner FailoverState 0:0:1 target ready 2FF70002AC01AE3E 20010002AC01AE3E host FC - 1:0:1 none 0:0:2 target ready 2FF70002AC01AE3E 20020002AC01AE3E host FC - 1:0:2 none 0:1:1 initiator ready 50002ACFF701AE3E 50002AC01101AE3E disk SAS DP-1 - - 0:1:2 initiator ready 50002ACFF701AE3E 50002AC01201AE3E disk SAS DP-2 - - 0:3:1 peer offline - 3464A9EAFD3D free IP IP0 - - 1:0:1 target ready 2FF70002AC01AE3E 21010002AC01AE3E host FC - 0:0:1 none 1:0:2 target ready 2FF70002AC01AE3E 21020002AC01AE3E host FC - 0:0:2 none 1:1:1 initiator ready 50002ACFF701AE3E 50002AC11101AE3E disk SAS DP-1 - - 1:1:2 initiator ready 50002ACFF701AE3E 50002AC11201AE3E disk SAS DP-2 - - 1:3:1 peer offline - 94188246CFDD free IP IP1 - - ----------------------------------------------------------------------------------------------------- % showportlesb hist 1:0:1 ID ALPA ----Port_WWN---- LinkFail LossSync LossSig PrimSeq InvWord InvCRC &amp;lt;1:0:1&amp;gt; 0x15e00 21010002AC01AE3E 2 3 0 0 163 19870 host42 0x15300 51402EC000F79136 2 0 0 0 0 0 host43 0x15400 51402EC000F77CFA 2 0 0 0 0 0 host44 0x15500 51402EC000F79236 2 0 0 0 0 0 host45 0x15600 51402EC000F77D06 2 0 0 0 0 0 host48 0x15900 51402EC000F77F0E 2 0 0 0 0 0 一般情况3PAR端showportlesb会显示哪个host报crc错误，这次直接报接在同一交换机上的两个控制器端口&amp;lt;1:0:1&amp;gt;和&amp;lt;0:0:1&amp;gt;故障，这让人不得不怀疑控制器其实是没有故障的，故障在交换机侧，毕竟不同控制器的端口同时报错概率不大。</description>
    </item>
    
    <item>
      <title>ansible tower升级</title>
      <link>https://suikaremilia.github.io/post/ansible-tower-upgrade/</link>
      <pubDate>Mon, 27 Jul 2020 17:39:01 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/ansible-tower-upgrade/</guid>
      <description>Ansible Tower的版本更新的也是莫名的快，去年安装的3.5今年已经到3.7.1了，新的版本可以在线申请license，旧的license快过期了，所以就升个级吧。
步骤： 升级的时候不能只看升级文档，还要看新版本的support，一开始只看了升级文档，做的时候报操作系统版本不被支持，所以第一步配个较新的rhel源，然后update系统。
# yum update -y # systemctl reboot 下载要升级的版本，并解压：
# wget https://releases.ansible.com/ansible-tower/setup/ansible-tower-setup-3.7.1-1.tar.gz # tar xvzf ansible-tower-setup-3.7.1-1.tar.gz 把曾经3.5时候安装的inventory拷贝一份到3.7,注意要把老版本的rabbitmq_host有关的变量更改为routable_hostname，因为新版本移除了rabbitmq：
# cp ansible-tower-bundle-3.5.1-1.el7/inventory ansible-tower-setup-3.7.1-1/ # cat inventory [tower] ansible01-ap ansible02-ap [database] [all:vars] admin_password=&amp;#39;redhat&amp;#39; pg_host=&amp;#39;10.xx.xx.xx&amp;#39; pg_port=&amp;#39;5432&amp;#39; pg_database=&amp;#39;towerdata&amp;#39; pg_username=&amp;#39;postgres&amp;#39; pg_password=&amp;#39;redhat&amp;#39; routable_username=&amp;#39;tower&amp;#39; routable_password=&amp;#39;redhat&amp;#39; routable_cookie=cookiemonster 因为涉及到clust所以用到了rhscl里的两个包，可以单独下下来建一个源，也可以直接陪rhscl的源，我enable了base和rhcsl两个源。 然后，在各节点上关闭服务：
ansible tower -m command -a &amp;#34;ansible-tower-service stop&amp;#34; -i inventory 拆除主备实例关系：
# awx-manage list_instances [tower capacity=154] ansible01-ap capacity=72 version=3.5.1 ansible02-ap capacity=72 version=3.5.1 # awx-manage deprovision_instance --hostname=ansible02-ap # awx-manage list_instances [tower capacity=0] ansible01-ap capacity=0 version=3.</description>
    </item>
    
    <item>
      <title>Set up proxy for podman</title>
      <link>https://suikaremilia.github.io/post/set-up-proxy-for-podman/</link>
      <pubDate>Mon, 20 Jul 2020 14:15:12 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/set-up-proxy-for-podman/</guid>
      <description>背景如题，方案在/etc/profile.d/下配置：
# cat /etc/profile.d/http_proxy.sh export HTTP_PROXY=http://192.168.0.1:8080 export HTTPS_PROXY=http://192.168.0.1:8080 或直接export，但前者对ansible也生效，后者只能是该终端的用户
貌似都是废话，其他程序配置Proxy，用常规的用export命令配置环境变量不生效的时候，也可以试试这样……</description>
    </item>
    
    <item>
      <title>RHEL挂载windows共享</title>
      <link>https://suikaremilia.github.io/post/rhel-mount-windows-share/</link>
      <pubDate>Mon, 13 Jul 2020 13:50:07 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/rhel-mount-windows-share/</guid>
      <description>莫名其妙的曾经能用的mount cifs的方式开始报错
mount error(112): Host is down Refer to the mount.cifs(8) manual page (e.g. man mount.cifs) 查了一下原因是smb协议版本问题，RHEL5和6最高支持SMB 1.0协议，RHEL7.2以后支持到SMB3。 所以，RHEL5和6就不要想挂载最新的windows共享了，windows目前关闭了smb1，RHEL7如果想访问windows共享的话，则需要指定一下版本。
mount -t cifs -o vers=2.0,username=&amp;lt;win_share_user&amp;gt;,password=&amp;lt;win_share_password&amp;gt; //WIN_SHARE_IP/&amp;lt;share_name&amp;gt; /mnt/win_share 如果是域用户，则加domain信息：
mount -t cifs -o vers=2.0,username=&amp;lt;win_share_user&amp;gt;,domain=&amp;lt;win_domain&amp;gt;，password=&amp;lt;win_share_password&amp;gt; //WIN_SHARE_IP/&amp;lt;share_name&amp;gt; /mnt/win_share 或者更安全一点的做法，建个文件储存用户名和密码：
# cat /etc/win-credentials username = user password = password domain = domain 设置权限： chown root: /etc/win-credentials chmod 600 /etc/win-credentials 挂载： mount -t cifs -o credentials=/etc/win-credentials //WIN_SHARE_IP/&amp;lt;share_name&amp;gt; /mnt/win_share 也可以把信息写入/etc/fstab来实现自动挂载：
# cat /etc/fstab # &amp;lt;file system&amp;gt; &amp;lt;dir&amp;gt; &amp;lt;type&amp;gt; &amp;lt;options&amp;gt; &amp;lt;dump&amp;gt; &amp;lt;pass&amp;gt; //WIN_SHARE_IP/share_name /mnt/win_share cifs credentials=/etc/win-credentials,file_mode=0755,dir_mode=0755 0 0 mount /mnt/win_share </description>
    </item>
    
    <item>
      <title>Ubuntu删除snap</title>
      <link>https://suikaremilia.github.io/post/ubuntu-delete-snap/</link>
      <pubDate>Tue, 07 Jul 2020 09:25:12 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/ubuntu-delete-snap/</guid>
      <description>Snap在ubuntu里简直是病毒一样的存在，一定要用purge才能删掉……
顽固病毒也不过如此
# apt autoremove --purge snapd 更彻底一点：
# rm -rf /var/cache/snapd # rm -rf ~/snap </description>
    </item>
    
    <item>
      <title>香港……</title>
      <link>https://suikaremilia.github.io/post/hongkong/</link>
      <pubDate>Tue, 30 Jun 2020 15:26:10 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/hongkong/</guid>
      <description>明天香港就真的归还给中共了，华人的净土没有了，虽然早知道会这样，却还是有点伤感……</description>
    </item>
    
    <item>
      <title>CEPH规划</title>
      <link>https://suikaremilia.github.io/post/ceph-architecture/</link>
      <pubDate>Wed, 20 May 2020 15:16:05 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/ceph-architecture/</guid>
      <description>背景 终于准备用Nautilus做生产环境了！
目前，Ceph 主要有三种企业级应用场景：
IOPS 密集型：这种类型的场景通常是支撑在虚拟化/私有云上运行数据库。如在 OpenStack 上运行 Mysql、MariaDB 或 PostgreSQL 等。IOPS 密集型场景对磁盘的性能要求较高，最好使用全闪架构。如果使用混合架构，机械盘转速需要 1.2 万，并使用高速盘存储频繁写操作的日志或元数据。 高吞吐量型：这种类型的应用场景主要是大块数据传输，如图像、视频、音频文件等。高吞吐量型磁盘的要求没有 IOPS 密集型高，但需要配置较高的网络。同时也需要配置 SSD 来处理写日志。 高容量型：这种场景主要用于存储归档、离线数据。它对磁盘的容量要求高，对性能无过多要求。写日志也可以存储在 HDD 上。
此次计划部署一套带宽型的用来提供S3服务，所以部署第二种就好。 硬件规划： nautilus采用blustore比filestore的性能大大提升，毕竟传输路径短了很多：
随之而来的问题是BlueStore 存储引擎的实现，需要存储数据和元数据。目前 Ceph BuleStore 的元数据存储在 RocksDB（K-V 数据库）中。通过为 RocksEnv 提供操作接口，RocksDB 存放在 BlueFS 上。由于 BlueFS 最终通过 RocksDB，承载的是 BlueStore 存储引擎中的元数据，因此它的性能会很大程度上影响整个 Ceph 的性能。所以，要为它提供高速硬盘。
Redhat建议，在新的bluestore架构下，如果用NVMe做metedata/jouranl，对应HDD的OSD是1：12；如果普通SSD做metedata/jouranl，则与HDD的比例为1：4 1 在此基础上，其他按照官方建议配置。
Device Size Count Use CPU Inter Xeon 6252 2 Memor 32GB DISK 4TB 12 OSD SSD 480GB 2 Operation NVMe 4TB 2 blockk.</description>
    </item>
    
    <item>
      <title>封装虚拟机</title>
      <link>https://suikaremilia.github.io/post/template/</link>
      <pubDate>Tue, 12 May 2020 11:19:16 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/template/</guid>
      <description>很多时候我们需要重复创建虚机，模板就是方便此类重复操作的一个虚机副本。模板不能拥有任何特定的信息，否则依此建立的虚机将都带有特定信息。所以，在建立模板前需要对源虚机进行封装，所谓封装就是在保证其他功能正常的同时，去除掉特点的功能。
以前linux通用的封装方式比较烦，需要手动删除如下：
ssh主机密钥： rm -rf /etc/ssh/ssh_host_* 设置hostname为localhost cat /etc/sysconfig/network HOSTNAME=localhost.localdomain 从/var/log中删除所有日志： for i in `find .-name &amp;#34;*.log&amp;#34;`;do cat /dev/null&amp;gt;$i;done /root中删除build日志： 关闭虚机 现在rhel提供了更简便的封装方法：
yum install -y libguestfs-tools virt-sysprep windows的封装一向很简单：
运行sysprep.exe
一般在C:\Windows\System32\sysprep\sysprep.exe
此类是通用的封装方法，有虚拟化平台提供自己的封装工具，那么以平台为准。</description>
    </item>
    
    <item>
      <title>虚拟机镜像转换</title>
      <link>https://suikaremilia.github.io/post/convert-image/</link>
      <pubDate>Fri, 08 May 2020 11:00:23 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/convert-image/</guid>
      <description>总会遇到诡异的问题，比如要部署厂家打包好的定制化的镜像，发现这镜像只提供一种平台的格式，这时候就需要将虚拟磁盘做转换了。
各个厂家提供一些定制化的工具并不是太通用，比如VMWare的vmware-vdiskmanager，好在这个世界有开源的qemu-img工具，可以方便的转换各种格式的镜像。
安装： qemu-img提供多平台支持，常见的windows可以从官网下载安装；linux就更简单了，配源然后install，比如：yum install qemu-img
参数： 常用的转换参数如下：
convert：转换操作；
-p：显示转换进度;
-f xxx：指明被转换镜像的格式，可以留空，但hyperV的vhd最好写明vpc否则可能不识别；
-O xxx：表明要转换成的格式；
info：查询信息
实例： vmdk转qcow2：
qemu-img convert -p -f vmdk -O qcow2 RHEL.vmdk RHEL.qcow2 查询镜像详细信息：
qemu-img info RHEL.qcow2 如果转vhd文件，注意-f后跟vpc而不是vhd
以上没有了……</description>
    </item>
    
    <item>
      <title>部署RHHI环境</title>
      <link>https://suikaremilia.github.io/post/rhhi/</link>
      <pubDate>Tue, 05 May 2020 13:57:26 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/rhhi/</guid>
      <description>一直不知道为啥红帽会把RHHI放进存储产品线，明明就是个虚拟化环境的超融合环境，甚至安装介质跟rhv都是一样的，两者只有部署方式上的差异。
从最开始的RHV一直用到现在的RHHI，红帽的产品没有VMWare那么丰富的接口，却也别有特色，用习惯了感觉也不错。才不是用多了国产的烂玩意才有的这种感觉。
这两天环境被弄乱了，就图省事不修了，重新部署了一套了事，记录一点过程：
首先，要有RHVH的订阅，没有就去下ovirt，rhv对应的开源版本就是它；
其次，红帽对此的文档已经很详细了，也不需要帐号就可以阅读，相关的两篇文档如下：
https://access.redhat.com/documentation/en-us/red_hat_hyperconverged_infrastructure_for_virtualization/1.7/html/deploying_red_hat_hyperconverged_infrastructure_for_virtualization/deployment_workflow#enabling-software-repositories
https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4-beta/html/administration_guide/sect-preparing_and_adding_nfs_storage
注意，第一篇部署文档里没提到的是宿主机的主机名需要被DNS解析，否则部署host-engin的时候会过不去地址检测。
第二篇文档要注意的是nfs的权限设置，不是随便哪个nfs都能在rhv环境中挂起来的。
然后，按照文档部署完成之后要在compute-&amp;gt;cluster里把enable gluster service的勾勾上，不然在storage-&amp;gt;volumes里管理不到先前创建的gluster卷。
最后，就可以慢慢用了，从rhv2到3再到如今的4.3，改变还是蛮大的，也明显越来越好用了。虽然不是主流产品，但是如果有超融合需求，不妨尝试一下，毕竟有免费的开源版本ovirt。
但是4.3的RHV有个最坑的地方，由于宿主机是基于RHEL7的系统，因此无法将RHEL8的系统做成模板，因为rhel8中xfs有新特性，而这点居然写在了RHV4.4的文档里，而不是4.3的……</description>
    </item>
    
    <item>
      <title>Commvault修改介质信息</title>
      <link>https://suikaremilia.github.io/post/commvault/</link>
      <pubDate>Mon, 27 Apr 2020 16:53:56 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/commvault/</guid>
      <description>做磁带转储的时候发生了一件很奇怪的事情，一盘LTO3的磁带被认成了LTO2，在LTO5的驱动器里读不出来。
当然读不出来很正常，毕竟只向下兼容两代读，可认成LTO2就很诡异了。
于是找了一下方法把它修改过来：
1、CV管理员登录：
cd到安装目录然后qlogin，按提示输入用户名和密码；
2、对要操作的介质执行：
QOperation.exe execscript -sn changeMediaType -si media -si &amp;ldquo;000231&amp;rdquo; -si &amp;ldquo;ULTRIUM V3&amp;rdquo;</description>
    </item>
    
    <item>
      <title>pvmove迁移数据</title>
      <link>https://suikaremilia.github.io/post/pvmove/</link>
      <pubDate>Sun, 12 Apr 2020 15:29:48 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/pvmove/</guid>
      <description>使用LVM的好处之一就是可以用pvmove来方便的在线迁移数据。用pvmove来迁移、合并数据比创建mirorr的方式要安全而方便，它会把要迁移的数据拆分成多分，并通过建立临时镜像的方式来移动数据，即使中断也有全部的数据，还能很方便的再次开始。
简单的用法： pvmove /dev/sdc1 将/dev/sdc1上的数据迁移到vg中的其他物理卷
pvmove -n MYLV /dev/sdc1 将MYLV迁移到物理卷/dev/sdc1
pvmove -b /dev/sdc1 /dev/sdd1 pvmove的最常用用法，在后台迁移/dev/sdc1的数据到/dev/sdd1
pvmove -i5 /dev/sdc1 查询pvmove的进度，每5s输出一次结果。可以随时执行以查询迁移进度，后面不指定迁移的磁盘则会显示所有pvmove的进度
lvs -a -o +devices 另一种查询迁移进度的方式
迁移完成后： vgreduce /dev/sda pvremove /dev/sda 迁移完成后即可将不用的物理卷从vg中剔除，把它的PV属性去掉
注意： 虽然可以在线迁移，但仍会增大CPU、MEMORY、IO的开销，所以尽量在不忙的时候做操作。
如果要迁移clvmd管理的集群LVM，则需要确保cmirrord服务安装并开启。</description>
    </item>
    
    <item>
      <title>Caddy翻墙</title>
      <link>https://suikaremilia.github.io/post/caddy-fucke-gfw/</link>
      <pubDate>Mon, 06 Apr 2020 10:09:28 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/caddy-fucke-gfw/</guid>
      <description>Caddy有个模块很好用，经过简单的配置即可以https或http2的方式跨越防火墙，不需要单独申请证书同时还有正常网站做伪装，配合浏览器switch omega插件，方便快捷的访问被墙的资源。
安装： curl https://getcaddy.com | bash -s personal http.forwardproxy 配置一个正常网站： 创建文件并赋权等杂项：
setcap &amp;#39;cap_net_bind_service=+ep&amp;#39; /usr/local/bin/caddy useradd -r -d /var/www -M -s /sbin/nologin caddy chown -R caddy:caddy /var/www mkdir /etc/caddy chown -R root:caddy /etc/caddy touch /etc/caddy/Caddyfile chown caddy:caddy /etc/caddy/Caddyfile mkdir /var/log/caddy chown caddy:caddy /var/log/caddy/ mkdir -p /var/www/xxx.yyy.zzz chown -R caddy:caddy /var/www mkdir -p /etc/ssl/caddy chown -R caddy:caddy /etc/ssl/caddy 编辑配置文件：
vim /etc/caddy/Caddyfile https://xxx.yyy.zzz { //xxx.yyy.zzz为可以解析到本机的域名，下同 log /var/log/caddy/xxx.log errors /var/log/caddy/xxx.error root /var/www/xxx.yyy.zzz gzip tls &amp;lt;xxx@xxxxx.</description>
    </item>
    
    <item>
      <title>Python 更改pip源</title>
      <link>https://suikaremilia.github.io/post/pip/</link>
      <pubDate>Wed, 25 Mar 2020 10:08:05 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/pip/</guid>
      <description>内网环境往往需要改pip为内网源地址，否则要配Proxy。
临时更改： pip install -i &amp;lt;server IP&amp;gt; --trusted-host &amp;lt;server IP&amp;gt; &amp;lt;package_name&amp;gt; 永久修改： linux修改 ~/.pip/pip.conf中的index-url和trusted-host，windows修改C:\Users\xxx\pip\pip.ini (没有就创建一个)
[global] index-url = pypi.python.org trusted-host = pypi.python.org pypi.org files.pythonhosted.org </description>
    </item>
    
    <item>
      <title>Vim替换</title>
      <link>https://suikaremilia.github.io/post/vim-s/</link>
      <pubDate>Fri, 20 Mar 2020 10:05:31 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/vim-s/</guid>
      <description>Vim可以在命令模式下使用substitute命令，将指定的字符串替换成目标字符串。
基本语法： Vim替换命令的基本语法是 :[range]s/目标字符串/替换字符串/[option]，其中range和option字段都可以缺省不填。
range: 表示搜索范围，默认表示当前行; 也可以是以逗号隔开的两个数字，例如1,20表示从第1到第20行; 也可以是特殊字符，与其他地方的默认习俗一致，其中%表示整个文件
s 即substitute的简写，表示执行替换字符串操作;
option: 表示操作类型，默认只对第一个匹配的字符进行替换； 常见字段值有：g(global)表示全局替换; c(comfirm)表示操作时需要确认; i(ignorecase)表示不区分大小写;
简单实例 替换当前行第一个 VIM 为 sakuya :s/VIM/sakuya/
替换当前行所有 VIM 为 sakuya :s/VIM/sakuya/g 替换第 n 行开始到最后一行中每一行的第一个 VIM 为 sakuya :n,$s/VIM/sakuya/ 替换第 n 行开始到最后一行中每一行所有 VIM 为 sakuya
:n,$s/VIM/sakuya/g 替换每一行的第一个 VIM 为 sakuya
:%s/VIM/sakuya/ 等同于： :g/VIM/s//sakuya/
替换每一行中所有 VIM 为 sakuya :%s/VIM/sakuya/g 等同于： :g/VIM/s//sakuya/g
可以使用 # 作为分隔符，此时中间出现的 / 不会作为分隔符，例如：替换当前行第一个 VIM/ 为 sakuya/ :s#VIM/#sakuya/# 全文的行首加入//字符，批量注释时非常有用，/进行转义后才可以替换 :%s/^/\/\//
将所有行尾多余的空格删除 :%s= *$==</description>
    </item>
    
    <item>
      <title>tar.xz解压</title>
      <link>https://suikaremilia.github.io/post/tarxz/</link>
      <pubDate>Mon, 16 Mar 2020 11:08:56 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/tarxz/</guid>
      <description>遇到个包xx.tar.xz打包的，感觉这格式很罕见，有点懵，所以查了一下要怎么解压，顺便把tar复习一下。
tar.xz解压： # yum install -y xz # xz -d ***.tar.xz # tar -xvf ***.tar 这种格式本质是打包了两层
也可以不装xz，直接使用 tar xvJf **.tar.xz来解压
Tar选项： c – 创建压缩文件 x – 解压文件 v – 显示进度. f – 文件名. t – 查看压缩文件内容. j – 通过bzip2归档 z –通过gzip归档 r – 在压缩文件中追加文件或目录 W – 验证压缩文件 </description>
    </item>
    
    <item>
      <title>解决UWP应用无法联网的三种方法</title>
      <link>https://suikaremilia.github.io/post/uwp-no-internet/</link>
      <pubDate>Thu, 12 Mar 2020 14:07:17 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/uwp-no-internet/</guid>
      <description>author: &amp;ldquo;Suika&amp;rdquo; title: &amp;ldquo;解决UWP应用无法联网的三种方法&amp;rdquo; date: &amp;ldquo;2020-03-12 14:07:17&amp;rdquo; description: &amp;ldquo;怎么会有UWP这种东西&amp;rdquo; categories: &amp;ldquo;Tech&amp;rdquo; tags:
&amp;ldquo;UWP&amp;rdquo; “Proxy” image: &amp;quot;&amp;quot; surface Pro 6又坏了，微软的硬件好垃圾，一年多坏了3次，再不会买了，影响工作效率。每次送修的结果就是所有东西都要重配一遍，这次是换机，更彻底一点。然后，又遇到了当初第一次用surface的时候遇到的问题——UWP应用联网。于是把备份的一篇文字直接拿来解决这个问题，再发一遍方便以后找。
从Arch到Win10各种不适应，最近这种不适应在一个始料未及的事情上表现的尤为明显——只要开了全局代理，微软自家的UWP程序就无法联网了。
对此，在下也是服气。
然而，发现问题不解决不是吾辈的作风，于是开始找各种解决方案，顺带了解了一下UWP。
这玩意出来也好几年了，一直不温不火的，以至于我从来没有注意到还有这么个玩意。据说所有UWP应用均运行在被称为App Container的虚拟沙箱环境中，其安全性胜于传统的EXE应用。但问题也出在这里，安全的同时它阻止了网络流量发送到本机（即loopback），即阻止了UWP应用访问 localhost。没有访问回环地址的权限，也意味着即便系统设置中启用了本地代理，UWP应用也无法访问本地代理服务器。
按照官方的解决方案，貌似可以用CheckNetIsolation命令来解决这个问题，MSDN轻描淡写的写着命令：
checknetisolation loopbackexempt -a -n= UWP应用容器名 checknetisolation loopbackexempt -a -p= UWP应用SID 可是谁告诉我UWP的SID去哪里找？UWP的应用容器名称又是啥？
有一种找SID的方法，通过注册表获取：
win+r的运行里输入Regedit打开注册表编辑器，然后定位到
HKEY_CURRENT_USERSoftwareClassesLocal Settings Software MicrosoftWindows CurrentVersion App Container Mappings 接着在左边的注册表项中找到你想解除网络隔离的应用，右边的 DisplayName 就是应用名称，而左边那一大串字符就是应用的SID值了。
SID值是人类可以理解的么？对此我可以骂人么？这是人干的事？我不能骂人么？那我还有什么话好说。
这方法还是放弃的好！
官方的解决方案如此复杂，按照问题发生后最简单的思路去解决，那就是关闭代理。
这简单粗暴的方法确实可行，唯一的问题是关闭代理这事在国内是我不能接受的。
接着搜索，互联网给出的答案是网络调试工具Fiddler，然后左上角的WinConfig按钮，排除不要的UWP应用。
可是，Fiddler这工具用来干这个岂不是太大材小用了？
最终，我找到了这个——Windows-Loopback-Exemption-Manager
Github上的项目，看来遭这问题困扰的人好多啊
https://github.com/tiagonmas/Windows-Loopback-Exemption-Manager
好了，这玩意跟Fiddler用法差不多，功能更单一，仅此而已。
哦，这东西还有一个坑，就是邮件，用这种方法邮件还是没办法添加Gmail不知道是为啥。
以上就是三种解决方案，其实只能算两种:
如果想简单粗暴，那么关闭代理是最直接的,这简直不能算解决方案；
不然，就Fiddle或是Windows-Loopback-Exemption-Manager这两者虽然有坑都还可以接受；
我想没有人愿意手动去一条条命令敲吧，光是找SID的工作就是非人类的。</description>
    </item>
    
    <item>
      <title>Arch Linux instal</title>
      <link>https://suikaremilia.github.io/post/arch-linux-install/</link>
      <pubDate>Sun, 08 Mar 2020 17:48:42 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/arch-linux-install/</guid>
      <description>主力的本子用ARCH工作已经几年了，一直没有问题，有问题也都是能解决的问题。所以是出于莫名其妙的原因吧，在这个假期把arch重装了一下。跟几年前有些许变化，趁还有日志，记录一下过程。
一、环境准备： 设置bios为UEFI启动，各主机情况不同，一般是开机按F2进主板进行设置，设置的同时在boot里将从其他设备启动的选项打开；
现在没有本子不是UEFI启动了吧，如果有就去看官方文档吧，这篇不适合做参考。
下载最新的arch linux的ISO镜像，然后制作启动U盘。
我是在现有的arch上用dd做的：
dd if=archlinux.iso of=/dev/sdb bs=1M 然后，用U盘引导启动，进入live CD的系统后再确认一下启动方式是什么：
# ls /sys/firmware/efi/efivars 如果此目录下有文件则为UEFI，否则请重新检查bios设置。
国内的主机默认美国键盘就好，不需要做任何更改。
二、连接到网络： 有线默认是dhcp的，插上网线自动获取地址即可，无线稍微复杂一点，可选的工具也很多，我习惯用netctl这工具。
检查网卡名称：
ip link 拷贝并修改配置文件：
# cp /etc/netctl/examples/wireless-wpa /etc/netctl/ihangzhou # vim /etc/netctl/ihangzhou 修改了以下标记了的内容：
Description=&amp;#39;A simple WPA encrypted wireless connection&amp;#39; Interface=~~wlp2s0~~ Connection=wireless Security=wpa IP=dhcp ESSID=&amp;#39;~~i-hangzhou~~&amp;#39; Key=&amp;#39;~~xxxxxx~~&amp;#39; wifi的密码 # Uncomment this if your ssid is hidden Hidden=*~~yes~~* 是否是隐藏的网络，如不是则在此行前面加#将其注释掉，默认注就是注释的 # Set a priority for automatic profile selection Priority=10 启用配置：
cd /etc/netctl/ netctl start ihangzhou 如果没有报错应该就会获取到地址，有报错就检查一下哪里错了。</description>
    </item>
    
    <item>
      <title>CEPH Nautilus Zabbix module Setup</title>
      <link>https://suikaremilia.github.io/post/ceph-nautilus-zabbix-module-setup/</link>
      <pubDate>Mon, 02 Mar 2020 15:13:58 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/ceph-nautilus-zabbix-module-setup/</guid>
      <description>CEPH可以通过mgr节点的zabbix模块实现监控
不同于普通的agent模式，CEPH自带的zabbix模块通过trapper模式工作，mgr节点收集集群信息并向Zabbix Server发送。
1、环境说明： 3节点CEPH集群，每个node都是mgr节点;
部署Nautilus 14.2.7；
Zabbix Server版本：4.2.1
2、安装部署： 在所有mgr节点上安装zabbix-sender
配置yum：
# rpm -Uvh https://repo.zabbix.com/zabbix/4.4/rhel/7/x86_64/zabbix-release-4.4-1.el7.noarch.rpm 安装sender：
# yum install zabbix-sender -y 3、配置zabbix模块： 在任意一个mgr节点上执行如下配置：
启用zabbix module：
# ceph mgr module enable zabbix 指定zabbix server，如果有多个可以用&amp;quot;,&amp;ldquo;隔开，并且可以添加&amp;rdquo;:&amp;ldquo;来指定服务器端口，此处只有最简单的配置：
# ceph zabbix config-set zabbix_host zabbix.test.com 指定zabbix_sender的位置：
# which zabbix_sender /usr/bin/zabbix_sender # ceph zabbix config-set zabbix_sender /usr/bin/zabbix_sender Configuration option zabbix_sender updated 指定identifier，这里很奇怪，只能是某个mgr节点的主机名，其他的可以设置成功但是发送失败，不知道我哪里配的有问题：
# ceph zabbix config-set identifier &amp;#34;ceph01.test.com&amp;#34; 后面如果有需要可以定义server的端口等，命令跟前面的相仿，这里就不配了。
查看一下当前的配置：
# ceph zabbix config-show 查看一下自带的监控模板，后面会用到：
# find / -iname zabbix_template.</description>
    </item>
    
    <item>
      <title>Created slice &amp;amp;amp; Starting Session</title>
      <link>https://suikaremilia.github.io/post/created-slice-andampamp-starting-session/</link>
      <pubDate>Wed, 26 Feb 2020 15:40:16 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/created-slice-andampamp-starting-session/</guid>
      <description>背景： RHEL7的日志（/var/log/message）里总有如下log，甚至很长一段时间内整个log都是这些内容：
example.com systemd: Created slice user-0.slice. example.com systemd: Starting Session 150 of user root. example.com systemd: Started Session 150 of user root. example.com systemd: Created slice user-0.slice. example.com systemd: Starting Session 151 of user root. example.com systemd: Started Session 151 of user root. 过滤： 照红帽的官方说法，这是居然是正常情况，只要用户登录就会记录这些日志，如果不想记录这些内容需要主动将其过滤掉，方法如下：
echo &amp;#39;if $programname == &amp;#34;systemd&amp;#34; and ($msg contains &amp;#34;Starting Session&amp;#34; or $msg contains &amp;#34;Started Session&amp;#34; or $msg contains &amp;#34;Created slice&amp;#34; or $msg contains &amp;#34;Starting user-&amp;#34; or $msg contains &amp;#34;Starting User Slice of&amp;#34; or $msg contains &amp;#34;Removed session&amp;#34; or $msg contains &amp;#34;Removed slice User Slice of&amp;#34; or $msg contains &amp;#34;Stopping User Slice of&amp;#34;) then stop&amp;#39; &amp;gt;/etc/rsyslog.</description>
    </item>
    
    <item>
      <title>写在开始的碎碎念</title>
      <link>https://suikaremilia.github.io/post/mumbler/</link>
      <pubDate>Thu, 20 Feb 2020 09:32:34 +0000</pubDate>
      
      <guid>https://suikaremilia.github.io/post/mumbler/</guid>
      <description>新开端 居家办公可以算是好长的一个假期，自工作以来再没想过能有这样长的一个假。
不能出门的日子有人会觉得无聊，可是我不会。上班、健身、做饭、学习、炒汇、阅读、补电影，被宅在家里的日子，每一天都感觉不够用。终于有大块的时间做自己想做的事情了，多幸福！
从这个意义上或许应该要感谢一下这次瘟疫？
在某个阳光明媚的午后突然就觉得应该记录一下这样难得的日子，继而就想起前年搭的那个环境。那是第一台VPS，为了榨取它的价值，不仅被我用来搭梯子，也被我用作记录生活和想法。陆陆续续写了大半年，后来到期了就没有续费，临终的时候在Telegraph做了个备份，算是给自己留给回忆。如今又有了记录的想法，为了避免上一次荒废的悲剧，我决定依赖Github来建静态站，这样可以最大限度的避免外因导致的前功尽弃。
于是，现在这个界面出现了。我将在此记录日常的想法、生活的点滴以及技术的学习，写下自己的文字，记录生命中的点滴。
唯愿此事能长久！
延续 不知不觉疫情已经第三个年头了，还是看不到尽头。好在偶尔还能想起这个记录的地方，还没有荒废。
可是，回头看看快做成技术笔记了，跟初衷背离挺远的。我更希望能有个地方长篇的记录想法，毕竟零星的短篇都记在Telegram Channel了。
所以，决定换个风格！
而且，Gridea已经用了两年多了，想换个能随时编辑的，因此就有了这次折腾，换到了HUGO，持续继承什么的最棒了。
Anyway，希望记录想法这事能持续！</description>
    </item>
    
  </channel>
</rss>
