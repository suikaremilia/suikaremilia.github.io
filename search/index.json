[{"content":"一个新上线的系统，50G的根分区用df看快满了，但业务说不可能有那么多数据。\n常规的思路，换du去看根下有什么大文件，可是du -sh /*的所有结果加起来都没有5G大。\n很自然就想到有进程删除了文件但没有释放，然后用lsof排查\n1  lsof | grep deleted   确实有几个进程，但是kill掉之后用df看，根分区还是满的。\n重启系统，依旧是……\n想了半天没有结果，最终发现是个lv挂载给了有数据的目录，而该目录在挂载lv之前就已经有数据了，所以是上线前检查不到位。\n解决过程不需要重启，只需要如下操作 把根分区挂载给/mnt：\n1  mount -o bind / /mnt   查看此时/mnt下的文件大小\n1  du sh /mnt/*   此时会发现/mnt/point/中的文件大到占据了绝大多数根分区的空间，于是，删除或移走/mnt/point/下的文件。\n至此问题解决。\n","date":"2022-01-14T16:48:40Z","permalink":"https://example.com/post/%E6%A0%B9%E5%88%86%E5%8C%BA%E5%BF%AB%E6%BB%A1%E4%BA%86/","title":"根分区快满了"},{"content":"shred -f -v -d /dev/*d*\n当遇到解决不掉的linux问题时不妨尝试一下上面的命令\n试一试又不会有什么坏处\n","date":"2022-01-11T19:28:43Z","permalink":"https://example.com/post/%E4%B8%8D%E5%A6%A8%E4%B8%80%E8%AF%95%E7%9A%84linux%E5%91%BD%E4%BB%A4/","title":"不妨一试的Linux命令"},{"content":"升级了rhel8.4，发现AD域的账户登不上去了。\n日志显示的问题出在：\n1 2 3 4 5  (2021-06-24 17:34:18): [krb5_child[39535]] [get_and_save_tgt] (0x0020): 1757: [-1765328370][KDC has no support for encryption type] (2021-06-24 17:34:18): [krb5_child[39535]] [map_krb5_error] (0x0020): 1849: [-1765328370][KDC has no support for encryption type] (2021-06-24 17:35:46): [krb5_child[39555]] [validate_tgt] (0x0020): TGT failed verification using key for [host/xxxxx@xxxx.COM]. (2021-06-24 17:35:46): [krb5_child[39555]] [get_and_save_tgt] (0x0020): 1757: [-1765328370][KDC has no support for encryption type] (2021-06-24 17:35:46): [krb5_child[39555]] [map_krb5_error] (0x0020): 1849: [-1765328370][KDC has no support for encryption type]   这是什么鬼？KDC不支持的加密类型？？？可是加密类型是default啊。\n1 2  # cat etc/crypto-policies/config DEFAULT   于是去查了一下，发现真的是加密类型变了，redhat是这么写的：\n1 2 3  By default, SSSD supports RC4, AES-128, and AES-256 Kerberos encryption types. RC4 encryption has been deprecated and disabled by default in RHEL 8, as it is considered less secure than the newer AES-128 and AES-256 encryption types. In contrast, Active Directory (AD) user credentials and trusts between AD domains support RC4 encryption and they might not support AES encryption types.   Ensuring support for common encryption types in AD and RHEL\nIdentity Management\n既然找到原因，那就好解决了：\nRHEL8.3及以后比较简单，一条命令：\n1  # update-crypto-policies --set DEFAULT:AD-SUPPORT   提示要重启，实际不重启也可以。\n之前的版本虽然不存在这个问题，但要修改默认加密策略的话要麻烦一点：\n1 2 3 4  # vim /etc/crypto-policies/back-ends/krb5.config 添加+rc4在后面 [libdefaults] permitted_enctypes = aes256-cts-hmac-sha1-96 aes256-cts-hmac-sha384-192 camellia256-cts-cmac aes128-cts-hmac-sha1-96 aes128-cts-hmac-sha256-128 camellia128-cts-cmac +rc4   具体见这里：修改方法 文档中说是8.2的最麻烦，但实际用8.0和8.1的方法改了也OK\n节末的教训是，多看文档总是没有坏处的。\n","date":"2021-06-25T17:27:15Z","permalink":"https://example.com/post/rhel8%E5%8A%A0%E5%AF%86%E7%AD%96%E7%95%A5%E6%94%B9%E5%8F%98/","title":"RHEL8加密策略改变"},{"content":"发生个诡异的故障，用户ssh登录可以成功，但是马上session就被关闭了。\n1 2 3 4  ssh root@xxxx Password: Last Login: Thu Jun 24 14:26:00 2021 from xxxxx Connection to xxxx closed.   就是ssh 加上 -vvv看到的也是这种现象，既明显登录成功了，但立即被关闭了。\n好在有其他人还有个session连在上面，看了日志有类似的报错：\n1 2 3  Accepted keyboard-interactive/pam for usr_ins from xxx port xxx ssh2 pam_limits(sshd:session): Could not set limit for \u0026#39;nofile\u0026#39;: Operation not permitted error: PAM: pam_open_session(): Permission denied   几经排查，发现是fs.nr_open的锅。有人改了/etc/security/limits.conf中的hard nofile，改的这个值比fs.nr_open大了，改小hard nofile或改大fs.nr_open就解决了。\n1  sysctl -w fs.nr_open = xxxx   好在有个高权限的session还活着，不然要重启挂光盘修改了。\n总结： 只要在/var/log/secure中看到如下类似报错：\n1 2 3 4 5 6 7 8 9 10  login: pam_unix(remote:session): session opened for user root by (uid=0) login: Permission denied login: pam_limits(remote:session): Could not set limit for \u0026#39;nofile\u0026#39;: Operation not permitted su: pam_limits(su:session): Could not set limit for \u0026#39;nofile\u0026#39;: Operation not permitted su: pam_unix(su:session): session opened for user root by test(uid=501) pam: gdm-password: pam_limits(gdm-password:session): Could not set limit for \u0026#39;nofile\u0026#39;: Operation not permitted pam: gdm-password: pam_unix(gdm-password:session): session opened for user root by (uid=0)   那么就是此类问题引起的。\n另外，fs.nr_open的默认值是1024*1024 = 1048576，最大值是由sysctl_nr_open_max在内核控制的，在X86服务器上是2147483584\n这又增强了对linux一切皆文件的理解，系统文件不能打开了session当然无法创建。\n","date":"2021-06-24T15:42:08Z","permalink":"https://example.com/post/too-many-open-files%E5%AF%BC%E8%87%B4%E7%99%BB%E5%BD%95%E5%A4%B1%E8%B4%A5/","title":"Too many open files导致登录失败"},{"content":"规划 架构： 服务器信息：    用途 主机名 IP     Master1 ocpmaster01.suika.com 10.90.16.21   Work1 ocpwork01.suika.com 10.90.16.22   Work2 ocpwork02.suika.com 10.90.16.23   bootstrap bootstrap.ocpsuika.com 10.90.18.19   bastion bastion.suika.com 10.90.18.16    域名信息：    用途 域名 地址 端口     Kubernetes API api.ocp.suika.com 10.90.18.19 6443   Kubernetes API api-int.suika.com 10.90.18.19 443   Routes *.apps.suika.com 10.90.18.19 443       域名 IP 端口 用途     console-openshift-console.apps.ocp.suika.com 10.90.16.21 443 OCP控制台页面   oauth-openshift.apps.ocp.suika.com 10.90.16.21 443 OCP oauth登录跳转页面   prometheus-k8s-openshift-monitoring.apps.ocp.suika.com 10.90.16.21 443 Prometheus   kibana-openshift-logging.apps.ocp.suika.com 10.90.16.21 443 Kibana   api.ocp.suika.com 10.90.16.21 6443 API    端口列表：    协议 端口 用途     TCP 2379-2380 etcd server, peer, and metrics ports   TCP 6443 Kubernetes API   TCP 10249-10259 The default ports that Kubernetes reserves   TCP 10256 openshift-sdn   TCP 9000-9999 Host level services, including the node exporter on ports 9100-9101 and the Cluster Version Operator on port 9099.   UDP 4789、6081 VXLAN and GENEVE   UDP 9000-9999 Host level services, including the node exporter on ports 9100-9101.   TCP/UDP 30000-32767 Kubernetes NodePort    环境准备： 离线资源： pull secret 下载地址\nOpenShift CLI（OC）下载地址\nOpenshift-install CLI 下载地址\nCoreOS image 下载地址\n安装OpenShift CLI并创建目录： 1 2 3 4 5  # tar xvzf oc-4.6.3-linux.tar.gz # tar xvzf openshift-install-linux-4.6.3.tar.gz # mv oc kubectl openshift-install /usr/local/bin # mkdir -p /opt/install # touch /opt/install/install-config.yaml   DNS配置： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  [root@bastion ~]# yum install dnsmasq -y [root@bastion ~]# cat /etc/dnsmasq.d/ocp.conf  address=/bastion.suika.com/10.90.18.16 address=/api.ocp.suika.com/10.90.18.16 address=/.apps.ocp.suika.com/10.90.18.16 address=/api-int.ocp.suika.com/10.90.18.16 address=/ocpmaster01.ocp.suika.com/10.90.16.21 address=/ocpmaster02.ocp.suika.com/10.90.16.22 address=/ocpmaster03.ocp.suika.com/10.90.16.23 address=/bootstrap.ocp.suika.com/10.19.18.19 address=/registry.suika.com/10.90.18.9 address=/oauth-openshift.ocp.suika.com/10.90.18.16 ptr-record=21.16.90.10.in-addr.arpa,ocpmaster01.ocp.suika.com ptr-record=22.16.90.10.in-addr.arpa,ocpmaster02.ocp.suika.com ptr-record=23.16.90.10.in-addr.arpa,ocpmaster03.ocp.suika.com ptr-record=19.18.90.10.in-addr.arpa,bootstrap.ocp.suika.com srv-host=_etcd-server-ssl._tcp.ocp.suika.com,etcd-0.ocp.suika.com,2380,10 srv-host=_etcd-server-ssl._tcp.ocp.suika.com,etcd-1.ocp.suika.com,2380,10 srv-host=_etcd-server-ssl._tcp.ocp.suika.com,etcd-2.ocp.suika.com,2380,10   启动服务：\n1 2 3 4 5 6 7 8  [root@bastion ~]# systemctl start dnsmasq.service [root@bastion ~]# systemctl status dnsmasq.service  ● dnsmasq.service - DNS caching server. Loaded: loaded (/usr/lib/systemd/system/dnsmasq.service; enabled; vendor preset: disabled) Active: active (running) since Fri 2020-12-11 23:03:19 CST; 2 weeks 3 days ago Main PID: 46638 (dnsmasq) CGroup: /system.slice/dnsmasq.service └─46638 /usr/sbin/dnsmasq -k   配置haproxy： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99  [root@bastion ~]# yum install haproxy -y [root@bastion ~]# vim /etc/haproxy/haproxy.cfg global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats #--------------------------------------------------------------------- # common defaults that all the \u0026#39;listen\u0026#39; and \u0026#39;backend\u0026#39; sections will # use if not designated in their block #--------------------------------------------------------------------- defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000 listen stats bind :9000 mode http stats enable stats uri / monitor-uri /healthz frontend openshift-api-server bind *:6443 default_backend openshift-api-server mode tcp option tcplog backend openshift-api-server balance source mode tcp server bootstrap 10.90.18.19:6443 check server master-0 10.90.16.21:6443 check server master-1 10.90.16.22:6443 check server master-2 10.90.16.23:6443 check frontend machine-config-server bind *:22623 default_backend machine-config-server mode tcp option tcplog backend machine-config-server balance source mode tcp server bootstrap 10.90.18.19:22623 check server master-0 10.90.16.21:22623 check server master-0 10.90.16.22:22623 check server master-0 10.90.16.23:22623 check frontend ingress-http bind *:80 default_backend ingress-http mode tcp option tcplog backend ingress-http balance source mode tcp server worker-1 10.90.16.22:80 check server worker-2 10.90.16.23:80 check server worker-3 10.90.16.21:80 check frontend ingress-https bind *:443 default_backend ingress-https mode tcp option tcplog backend ingress-https balance source mode tcp server worker-1 10.90.16.22:443 check server worker-2 10.90.16.23:443 check server worker-3 10.90.16.21:443 check   查看服务状态：\n1 2 3 4 5 6 7 8 9 10 11  [root@bastion ~]# systemctl start haproxy.service [root@bastion ~]# systemctl status haproxy.service ● haproxy.service - HAProxy Load Balancer Loaded: loaded (/usr/lib/systemd/system/haproxy.service; enabled; vendor preset: disabled) Active: active (running) since Fri 2020-12-11 22:55:58 CST; 2 weeks 3 days ago Main PID: 46295 (haproxy-systemd) CGroup: /system.slice/haproxy.service ├─46295 /usr/sbin/haproxy-systemd-wrapper -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid ├─46296 /usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -Ds └─46297 /usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -Ds   配置http服务器： 1 2 3  [root@bastion ~]# yum install httpd -y [root@bastion ~]# mkdir -p /var/www/html/install/ [root@bastion ~]# ln -s /opt/install/ /var/www/html/install   同样启动服务并检查状态：\n1  [root@bastion ~]# systemctl start httpd.service；systemctl status httpd.service   安装过程： 生成密钥待用： 1  ssh-keygen -t rsa -b 2048 -N \u0026#34;\u0026#34; -f /root/.ssh/id_rsa ；cat /root/.ssh/id_rsa.pub   编辑install文件： 把下载的pull-secret.txt及上面的文件加到默认的install-config.yaml后面，并且配置proxy\n注意：noproxy中按“，”分隔\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  [root@bastion ~]# cat /opt/install-config.yaml apiVersion:v1baseDomain:suika.com proxy:httpProxy:http://suika:suika@proxy@suika.com:8080 noProxy:.suika.com,.suika.com,.ocp.suika.com,10.90.16.0/22compute:- hyperthreading:Enabled name:workerreplicas:0controlPlane:hyperthreading:Enabled name:master replicas:3metadata:name:ocpnetworking:clusterNetwork:- cidr:10.128.0.0/14 hostPrefix:23networkType:OpenShiftSDNserviceNetwork:- 172.30.0.0/16platform:none:{}fips:falsepullSecret:\u0026#39;{\u0026#34;auths\u0026#34;:{\u0026#34;cloud.openshift.com\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;b3BlbnNoaWZ0LXJlbGVhc2UtZGV2K2N1aXNvbmd0YW9oM2Njb20xdnY2YjdzZ3FodWxyYjBiYXhucThxbW51b2s6WUYyQ1I1UTNNV1NFU1dLVVNEME8zNUo4UkVRUjI3QVBURDVIUkVWRVpEUExRSUJFOE85U0dFWE81QlRQNVo2VQ==\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;suika@suika.com\u0026#34;},\u0026#34;quay.io\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;b3BlbnNoaWZ0LXJlbGVhc2UtZGV2K2N1aXNvbmd0YW9oM2Njb20xdnY2YjdzZ3FodWxyYjBiYXhucThxbW51b2s6WUYyQ1I1UTNNV1NFU1dLVVNEME8zNUo4UkVRUjI3QVBURDVIUkVWRVpEUExRSUJFOE85U0dFWE81QlRQNVo2VQ==\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;suika@suika.com\u0026#34;},\u0026#34;registry.connect.redhat.com\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;NzYxMDE1Mnx1aGMtMVZWNmI3c0dxaFVMcmIwYkF4blE4cU1OVU9LOmV5SmhiR2NpT2lKU1V6VXhNaUo5LmV5SnpkV0lpT2lJMU1tWTROR05tWm1Nd01EQTBOelV5WVdVNU16WmxaVGhrTkROaFlUbGhNeUo5LlJiT1g3Ml9qY21fVTZLSFdVMS1VT19YTUk0aXVuSXZ3SVRWa1dsUDZ2QlU0Qzl0U2pickltM1FuY2Q4WENjeUVoQWY5cUoxVXRtWHlVWk5JODZ3dEo1eVE1STZnU3FlZ2RBMEZ2ek1tQk9qUzZ6eHAxVm83ZmNfVG1oYzBQSERhSWVudlhTZ3BscnNaUWpRckllVUNlUlNuaGRGdDFSdHU0djFaMDh1V1N4Y3k4VGI0dXIyNmRXZWhUSUF6Q0R2aFNkRzBhUVZaRExOYVd5QTE2YjIwSEdFYVNRNmlXUDNQZ05tUWFmTkdyUnNOTFZYd01Qd0ZQU01YMkdsVGhmMXludEpiTXl3UEx1a3pzaEozeFJ3bkFFV0JIdGd2dVlDNUN2YjB1SFlUMDJWVUV1Q21hbDBBQkppUlo4LVdiOV9RZEgxcXNKQzBGRGFUa2dhSjhzSWpleE9qTWtRWVZCeHI4OGpNNVcta0xlbmhLQmRxMTladHFiWTdwMGd0S0JjWjFQczlkSWJRRzJKSTVUZEZza2suika1VDeHdYM1o2QVlBeGJyR2ItSHVBRi1PT0pTdVBKU3RNMDJPblBHdGVRaUVSd0ZOMUZNZjEwRmxjQktlRTIyUGR1SU1sN3gyWTIxalJMR3VZdHBZa0FWVUZ1UEdMM3l3ZVFsWGk4RWYxVG9FV2kwU0dpWXFaNEt1ZWNLVjhUWFhWNHJCUTJSNHhZQmVwZ1F5d2xPNWdiU0xNQ0IwNzkxUlAtRXFyUlNadkJhbERjSlRfbzNxNlk5VWYtYnRuaVBjeTY0bkIxUHZmakxwazNqY0tBdWtBVHZDZjhrUmJKRVkwOFVWQXVpZENadFZiZDByWXpXNS1TUDN3VC10ZGhTb2h5OWNvZ2xTWkZ6aXhlYmJFdkNmalBJ\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;suika@suika.com\u0026#34;},\u0026#34;registry.redhat.io\u0026#34;:{\u0026#34;auth\u0026#34;:\u0026#34;NzYxMDE1Mnx1aGMtMVZWNmI3c0dxaFVMcmIwYkF4blE4cU1OVU9LOmV5SmhiR2NpT2lKU1V6VXhNaUo5LmV5SnpkV0lpT2lJMU1tWTROR05tWm1Nd01EQTBOelV5WVdVNU16WmxaVGhrTkROaFlUbGhNeUo5LlJiT1g3Ml9qY21fVTZLSFdVMS1VT19YTUk0aXVuSXZ3SVRWa1dsUDZ2QlU0Qzl0U2pickltM1FuY2Q4WENjeUVoQWY5cUoxVXRtWHlVWk5JODZ3dEo1eVE1STZnU3FlZ2RBMEZ2ek1tQk9qUzZ6eHAxVm83ZmNfVG1oYzBQSERhSWVudlhTZ3BscnNaUWpRckllVUNlUlNuaGRGdDFSdHU0djFaMDh1V1N4Y3k4VGI0dXIyNmRXZWhUSUF6Q0R2aFNkRzBhUVZaRExOYVd5QTE2YjIwSEdFYVNRNmlXUDNQZ05tUWFmTkdyUnNOTFZYd01Qd0ZQU01YMkdsVGhmMXludEpiTXl3UEx1a3pzaEozeFJ3bkFFV0JIdGd2dVlDNUN2YjB1SFlUMDJWVUV1Q21hbDBBQkppUlo4LVdiOV9RZEgxcXNKQzBGRGFUa2dhSjhzSWpleE9qTWtRWVZCeHI4OGpNNVcta0xlbmhLQmRxMTladHFiWTdwMGd0S0JjWjFQczlkSWJRRzJKSTVUZEZza2suika1VDeHdYM1o2QVlBeGJyR2ItSHVBRi1PT0pTdVBKU3RNMDJPblBHdGVRaUVSd0ZOMUZNZjEwRmxjQktlRTIyUGR1SU1sN3gyWTIxalJMR3VZdHBZa0FWVUZ1UEdMM3l3ZVFsWGk4RWYxVG9FV2kwU0dpWXFaNEt1ZWNLVjhUWFhWNHJCUTJSNHhZQmVwZ1F5d2xPNWdiU0xNQ0IwNzkxUlAtRXFyUlNadkJhbERjSlRfbzNxNlk5VWYtYnRuaVBjeTY0bkIxUHZmakxwazNqY0tBdWtBVHZDZjhrUmJKRVkwOFVWQXVpZENadFZiZDByWXpXNS1TUDN3VC10ZGhTb2h5OWNvZ2xTWkZ6aXhlYmJFdkNmalBJ\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;suika@suika.com\u0026#34;}}}\u0026#39;sshKey:\u0026#39;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDnfEBthXuNhuE/3dnGHEfzB9I2aYVmJUMe25sxL9BsX02tj+OA4H30j30tB4hD+AWrP3PEnt8zdD25ggGu1lGE4Ckt38uhMYud/a+amr5oTcIfIYlj/f9mo8VwKzehx7K7G8JWnFXwPjln97MuM/QVg6S+DY2wvGCSG+6MFwrq4/OCBlu1qjyoaE6yMl510n3rOB1WrUcj/LCiHBTsCsytx4fmtbSLxUxMyl3nfA83zhATg+meHKFtLpIEGZbMib7diI8APcOlEX4lOOwqf0lI6bFfTy2pRJ2cLAJXLvQaJL4eOhTOMqzbquj3g+gzkqpMf2R9L/fDTP5pwQqIkJjd root@bastion\u0026#39;[root@bastion ~]# cp /opt/install-config.yaml /opt/install/install-config.yaml   创建配置： 1 2 3 4 5 6 7 8 9 10 11 12  [root@bastion ~]# openshift-install create manifests --dir=/opt/install INFO Consuming Install Config from target directory WARNING Making control-plane schedulable by setting MastersSchedulable to true for Scheduler cluster settings INFO Manifests created in: /opt/install/manifests and /opt/install/openshift [root@bastion ~]# openshift-install create ignition-configs --dir=/opt/install/ INFO Consuming Common Manifests from target directory INFO Consuming Master Machines from target directory INFO Consuming OpenShift Install (Manifests) from target directory INFO Consuming Openshift Manifests from target directory INFO Consuming Worker Machines from target directory INFO Ignition-Configs created in: /opt/install and /opt/install/auth   现在的目录结构是这样：\n1 2 3 4 5 6 7 8 9 10 11 12 13  [root@bastion ~]# tree /opt/install /opt/install ├── auth │ ├── kubeadmin-password │ └── kubeconfig ├── bootstrap.ign ├── bootstrap.sh ├── master.ign ├── master.sh ├── metadata.json └── worker.ign 1 directory, 8 files   创建安装脚本： 环境比较特殊，不能搭DHCP所以无法pxe安装，只能配置静态IP，又懒得改镜像，所以就琢磨出这么个办法安装。\n注意脚本中的'rd.neednet=1'否则启动的时候有可能主机名变为localhost\n1 2 3 4 5 6 7  [root@bastion ~]# cat /var/www/html/install/bootstrap.sh #!/bin/bash /bin/coreos-installer install --copy-network --insecure-ignition --ignition-url=http://10.90.18.16:8080/install/bootstrap.ign --firstboot-args \u0026#39;rd.neednet=1\u0026#39; /dev/vda [root@bastion ~]# cat /var/www/html/install/master.sh #!/bin/bash /bin/coreos-installer install --copy-network --insecure-ignition --ignition-url=http://10.90.18.16:8080/install/master.ign --firstboot-args \u0026#39;rd.neednet=1\u0026#39; /dev/sda   启动设备： 打开设备电源并在ilo挂载rhcos镜像：\n用nmtui配置网络，生效后检查是否获取到主机名\n下载上面的安装脚本进行安装：\n重启设备进行安装： 安装完成后先重启bootstrap，待其重启完成后，并检查有pod后重启三台master主机。\n1 2 3 4 5 6 7 8 9 10 11 12 13  [core@bootstrap ~]$ sudo crictl pods POD ID CREATED STATE NAME NAMESPACE ATTEMPT 519ca2107ede7 33 seconds ago Ready bootstrap-kube-scheduler-bootstrap.ocp.suika.com kube-system 0 ee3682d35720d 33 seconds ago Ready bootstrap-kube-controller-manager-bootstrap.ocp.suika.com kube-system 0 4832d0632a339 33 seconds ago Ready bootstrap-kube-apiserver-bootstrap.ocp.suika.com kube-system 0 1e6aa212b904d 33 seconds ago Ready cloud-credential-operator-bootstrap.ocp.suika.com openshift-cloud-credential-operator 0 4fc5ec6c6e443 33 seconds ago Ready bootstrap-cluster-version-operator-bootstrap.ocp.suika.com openshift-cluster-version 0 fee62b862de12 50 seconds ago Ready bootstrap-machine-config-operator-bootstrap.ocp.suika.com default 0 f7b83eedcfbc6 About a minute ago Ready etcd-bootstrap-member-bootstrap.ocp.suika.com openshift-etcd 0 [core@bootstrap ~]$ ss -tulnp|grep 6443 tcp LISTEN 0 128 *:6443 *:* [core@bootstrap ~]$ ss -tulnp|grep 22623 tcp LISTEN 0 128 *:22623 *:*   重启master的时候需要盯着点ilo，启动完成后虽然可以获取到正确的主机名，但还是要把主机名修改一下，不然后面再重启主机名可能又变回localhost了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  [root@bastion~]# ssh core@10.90.16.23 Theauthenticityofhost\u0026#39;10.90.16.23 (10.90.16.23)\u0026#39;can\u0026#39;t be established. ECDSA key fingerprint is SHA256:9ZehCh3XkLlk86xu43DiSnIk8bT14UfPGc+dlxX6qzg. ECDSA key fingerprint is MD5:a7:d7:2c:58:5d:8c:42:44:ec:fa:b0:b8:76:86:7f:14. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added \u0026#39;10.90.16.23\u0026#39; (ECDSA) to the list of known hosts. Red Hat Enterprise Linux CoreOS 46.82.202010091720-0 Part of OpenShift 4.6, RHCOS is a Kubernetes native operating system managed by the Machine Config Operator (`clusteroperator/machine-config`). WARNING: Direct SSH access to machines is not recommended; instead, make configuration changes via `machineconfig` objects: https://docs.openshift.com/container-platform/4.6/architecture/architecture-rhcos.html --- [core@ocpmaster03 ~]$ hostname ocpmaster03.ocp.suika.com [core@ocpmaster03 ~]$ sudo hostnamectl set-hostname ocpmaster03.ocp.suika.com   等待安装完成： 此后剩下的就是等待安装完成，大概要一杯咖啡的时间：\n1 2 3 4 5 6  [root@bastion ~]# openshift-install --dir=/opt/install wait-for bootstrap-complete --log-level=debug DEBUG OpenShift Installer 4.6.4 DEBUG Built from commit 6e02d049701437fa81521fe981405745a62c86c5 INFO Waiting up to 20m0s for the Kubernetes API at https://api.ocp.suika.com:6443... INFO API v1.19.0+9f84db3 up INFO Waiting up to 30m0s for bootstrapping to complete...   至此可以移除bootstrap节点，并把haproxy中关于bootstrap的信息注释掉，并重启haproxy，然后继续等待安装完成：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  [root@bastion ~]# oc get node NAME STATUS ROLES AGE VERSION ocpmaster01.ocp.suika.com Ready master,worker 2m v1.19.0+9f84db3 ocpmaster02.ocp.suika.com Ready master,worker 5m9s v1.19.0+9f84db3 ocpmaster03.ocp.suika.com Ready master,worker 10m v1.19.0+9f84db3 [root@bastion install]# openshift-install --dir=/opt/install wait-for install-complete --log-level debug DEBUG OpenShift Installer 4.6.4 DEBUG Built from commit 6e02d049701437fa81521fe981405745a62c86c5 DEBUG Loading Install Config... DEBUG Loading SSH Key... DEBUG Loading Base Domain... DEBUG Loading Platform... DEBUG Loading Cluster Name... DEBUG Loading Base Domain... DEBUG Loading Platform... DEBUG Loading Pull Secret... DEBUG Loading Platform... DEBUG Using Install Config loaded from state file INFO Waiting up to 40m0s for the cluster at https://api.ocp.suika.com:6443 to initialize... DEBUG Cluster is initialized INFO Waiting up to 10m0s for the openshift-console route to be created... DEBUG Route found in openshift-console namespace: console DEBUG Route found in openshift-console namespace: downloads DEBUG OpenShift console route is created INFO Install complete! INFO To access the cluster as the system:admin user when using \u0026#39;oc\u0026#39;, run \u0026#39;export KUBECONFIG=/opt/install/auth/kubeconfig\u0026#39; INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp.suika.com INFO Login to the console with user: \u0026#34;kubeadmin\u0026#34;, and password: \u0026#34;XviuD-bnp8Y-RNYxL-zcp8R\u0026#34; INFO Time elapsed: 0s   注意这个输出的kubeadmin用户及其密码，后面console登录会用到\n安装后的工作： 到上面那个输出openshift就已经安装完成了，做一点额外的工作\n检查状态： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  [root@bastion ~]# oc get co NAME VERSION AVAILABLE PROGRESSING DEGRADED SINCE authentication 4.6.4 True False False 90s cloud-credential 4.6.4 True False False 37m cluster-autoscaler 4.6.4 True False False 21m config-operator 4.6.4 True False False 22m console 4.6.4 True False False 8m50s csi-snapshot-controller 4.6.4 True False False 22m dns 4.6.4 True False False 20m etcd 4.6.4 True False False 15m image-registry 4.6.4 True False False 12m ingress 4.6.4 True False False 12m insights 4.6.4 True False False 22m kube-apiserver 4.6.4 True False False 15m kube-controller-manager 4.6.4 True False False 20m kube-scheduler 4.6.4 True False False 16m kube-storage-version-migrator 4.6.4 True False False 20m machine-api 4.6.4 True False False 21m machine-approver 4.6.4 True False False 21m machine-config 4.6.4 True False False 20m marketplace 4.6.4 True False False 20m monitoring 4.6.4 True False False 10m network 4.6.4 True False False 23m node-tuning 4.6.4 True False False 22m openshift-apiserver 4.6.4 True False False 11m openshift-controller-manager 4.6.4 True False False 19m openshift-samples 4.6.4 True False False 12m operator-lifecycle-manager 4.6.4 True False False 21m operator-lifecycle-manager-catalog 4.6.4 True False False 21m operator-lifecycle-manager-packageserver 4.6.4 True False False 12m service-ca 4.6.4 True False False 22m storage 4.6.4 True False False 22m   获取console地址： 1 2 3 4 5 6 7 8 9 10  [root@bastion ~]# oc get route -A |grep openshift openshift-authentication oauth-openshift oauth-openshift.apps.ocp.suika.com oauth-openshift 6443 passthrough/Redirect None openshift-cnv test-maven-app test-maven-app-openshift-cnv.apps.ocp.suika.com test-maven-app 8080-tcp None openshift-console console console-openshift-console.apps.ocp.suika.com console https reencrypt/Redirect None openshift-console downloads downloads-openshift-console.apps.ocp.suika.com downloads http edge/Redirect None openshift-monitoring alertmanager-main alertmanager-main-openshift-monitoring.apps.ocp.suika.com alertmanager-main web reencrypt/Redirect None openshift-monitoring grafana grafana-openshift-monitoring.apps.ocp.suika.com grafana https reencrypt/Redirect None openshift-monitoring prometheus-k8s prometheus-k8s-openshift-monitoring.apps.ocp.suika.com prometheus-k8s web reencrypt/Redirect None openshift-monitoring thanos-querier thanos-querier-openshift-monitoring.apps.ocp.suika.com thanos-querier web reencrypt/Redirect None openshift nodejs-sample nodejs-sample-openshift.apps.ocp.suika.com nodejs-sample 8080-tcp None   添加上面的域名解析到任意一个master或haproxy的虚地址，然后就可以用web登录了，用前面的kubeadmin用户及密码\n配置时钟同步服务： 通过浏览器登录到open shift的console之后，就会发现一个告警，集群没有配置NTP服务，然而又翻了一遍安装文档还是没地方讲这玩意怎么配。\n找了一下红帽的官网，发现解决的办法有两个，一个就是在主机启动后尽快登录，然后配置ntp服务，就是在重启后修改主机名的时候一并修改了NTP，然而这是什么鬼方案，如果不是环境不允许谁没事盯着ilo看启动到哪一步了……\n合理的是采用第二种方案：\n1、创建base64格式的chrony.conf\n1 2 3 4 5 6 7  cat \u0026lt;\u0026lt; EOF | base64 -w 0 server suikantp02-in.suika.com iburst driftfile /var/lib/chrony/drift makestep 1.0 3 rtcsync logdir /var/log/chrony EOF   2、创建99_masters-chrony-configuration.yaml文件，并写入上面的信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  [root@bastion opt]# cat 99_masters-chrony-configuration.yaml apiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:labels:machineconfiguration.openshift.io/role:mastername:masters-chrony-configurationspec:config:ignition:config:{}security:tls:{}timeouts:{}version:2.2.0networkd:{}passwd:{}storage:files:- contents:source:data:text/plain;charset=utf-8;base64,c2VydmVyIGgzY250cDAyLWluLmgzYy5jb20gaWJ1cnN0CmRyaWZ0ZmlsZSAvdmFyL2xpYi9jaHJvbnkvZHJpZnQKbWFrZXN0ZXAgMS4wIDMKcnRjc3luYwpsb2dkaXIgL3Zhci9sb2cvY2hyb255Cg==verification:{}filesystem:rootmode:420path:/etc/chrony.confosImageURL:\u0026#34;\u0026#34;  如果有worker节点创建一个类似的99_workers-chrony-configuration.yaml的文件。\n3、应用文件：\n1  oc apply -f 99_masters-chrony-configuration.yaml   等一会，登录到任意主机，检查一下chrony的状态已是修改后的状态：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  [core@ocpmaster03 ~]$ cat /etc/chrony.conf server suikantp02-in.suika.com iburst driftfile /var/lib/chrony/drift makestep 1.0 3 rtcsync logdir /var/log/chrony [core@ocpmaster03 ~]$ timedatectl Local time: Mon 2020-12-14 08:12:01 UTC Universal time: Mon 2020-12-14 08:12:01 UTC RTC time: Mon 2020-12-14 08:12:01 Time zone: UTC (UTC, +0000) System clock synchronized: yes NTP service: active RTC in local TZ: no   至此，集群搭建完毕。\n参考文档： https://access.redhat.com/solutions/4906341\nhttps://misa.gitbook.io/k8s-ocp-yaml/openshift-docs/\nhttps://docs.openshift.com/container-platform/4.6/installing/installing_bare_metal/installing-bare-metal.html\n","date":"2020-12-12T09:14:40Z","permalink":"https://example.com/post/opne-shift-4.6%E5%9C%A8%E7%BA%BF%E5%AE%89%E8%A3%85/","title":"Opne Shift 4.6在线安装"},{"content":"最近一台3PAR一直报CRC错误，3PAR的排障还算简单，就几条命令分析一下输出。\n然而，这次故障比较诡异：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  % showport N:S:P Mode State ----Node_WWN---- -Port_WWN/HW_Addr- Type Protocol Label Partner FailoverState 0:0:1 target ready 2FF70002AC01AE3E 20010002AC01AE3E host FC - 1:0:1 none 0:0:2 target ready 2FF70002AC01AE3E 20020002AC01AE3E host FC - 1:0:2 none 0:1:1 initiator ready 50002ACFF701AE3E 50002AC01101AE3E disk SAS DP-1 - - 0:1:2 initiator ready 50002ACFF701AE3E 50002AC01201AE3E disk SAS DP-2 - - 0:3:1 peer offline - 3464A9EAFD3D free IP IP0 - - 1:0:1 target ready 2FF70002AC01AE3E 21010002AC01AE3E host FC - 0:0:1 none 1:0:2 target ready 2FF70002AC01AE3E 21020002AC01AE3E host FC - 0:0:2 none 1:1:1 initiator ready 50002ACFF701AE3E 50002AC11101AE3E disk SAS DP-1 - - 1:1:2 initiator ready 50002ACFF701AE3E 50002AC11201AE3E disk SAS DP-2 - - 1:3:1 peer offline - 94188246CFDD free IP IP1 - - ----------------------------------------------------------------------------------------------------- % showportlesb hist 1:0:1 ID ALPA ----Port_WWN---- LinkFail LossSync LossSig PrimSeq InvWord InvCRC \u0026lt;1:0:1\u0026gt; 0x15e00 21010002AC01AE3E 2 3 0 0 163 19870 host42 0x15300 51402EC000F79136 2 0 0 0 0 0 host43 0x15400 51402EC000F77CFA 2 0 0 0 0 0 host44 0x15500 51402EC000F79236 2 0 0 0 0 0 host45 0x15600 51402EC000F77D06 2 0 0 0 0 0 host48 0x15900 51402EC000F77F0E 2 0 0 0 0 0   一般情况3PAR端showportlesb会显示哪个host报crc错误，这次直接报接在同一交换机上的两个控制器端口\u0026lt;1:0:1\u0026gt;和\u0026lt;0:0:1\u0026gt;故障，这让人不得不怀疑控制器其实是没有故障的，故障在交换机侧，毕竟不同控制器的端口同时报错概率不大。\n于是，清除3par侧的计数\n1  showportlesb reset   继续排查交换机侧：\n1 2 3 4 5 6 7  \u0026gt; porterrshow  frames enc crc crc too too bad enc disc link loss loss frjt fbsy c3timeout pcs tx rx in err g_eof shrt long eof out c3 fail sync sig tx rx err 0: 463.2m 201.1m 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  1: 462.4m 200.9m 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 85: 32.6m 56.0m 424 423 423 0 0 0 0 0 0 0 0 0 0 0 0 0  86: 4.6m 4.9m 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0   果然一个主机的端口报错，考虑端口光衰\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  \u0026gt; sfpshow 85 Identifier: 3 SFP Connector: 7 LC Transceiver: 540c404000000000 2,4,8_Gbps M5,M6 sw Short_dist Encoding: 1 8B10B Baud Rate: 85 (units 100 megabaud) Length 9u: 0 (units km) Length 9u: 0 (units 100 meters) Length 50u (OM2): 5 (units 10 meters) Length 50u (OM3): 0 (units 10 meters) Length 62.5u:2 (units 10 meters) Length Cu: 0 (units 1 meter) Vendor Name: BROCADE Vendor OUI: 00:05:1e Vendor PN: 57-1000012-01 Vendor Rev: A Wavelength: 850 (units nm) Options: 003a Loss_of_Sig,Tx_Fault,Tx_Disable BR Max: 0 BR Min: 0 Serial No: UAF109390000C3N Date Code: 090924 DD Type: 0x68 Enh Options: 0xfa Status/Ctrl: 0x82 Alarm flags[0,1] = 0x5, 0x40 Warn Flags[0,1] = 0x5, 0x40 Alarm Warn low high low high Temperature: 42 Centigrade -10 90 -5 85 Current: 7.140 mAmps 1.000 17.000 2.000 14.000 Voltage: 3292.4 mVolts 2900.0 3700.0 3000.0 3600.0 RX Power: -2.3 dBm (582.9uW) 10.0 uW 1258.9 uW 15.8 uW 1000.0 uW TX Power: -3.3 dBm (263.1 uW) 125.9 uW 631.0 uW 158.5 uW 562.3 uW State transitions: 1 Last poll time: 08-03-2020 UTC Thu 08:12:57   TX Power才200多，过低，换之，遂好……\n附：博科交换机常用的内容如下： 默认IP:\n10.77.77.77\n默认密码： admin/password\nroot/fibrane\n查看、设置网络：\nipaddrshow\nipaddrset\n显示、添加license：\nlicenseshow\nlicenseadd\n查看信息：\nswitchshow\nalishow\nzoneshow\ncfgshow\nfabrishow\n配置时钟：\ntsclockserver \u0026ldquo;ntp server\u0026rdquo;\ntstimezone \u0026ndash;interactive\n清除状态：\nstatsclear\n排障的话就需要用到porterrshow，上面有它的输出，配合statsclear会得到一定时间范围内所有端口错误统计信息：\nFrame(tx/rx)： tx代表端口发送的数据帧，rx代表端口收到的数据帧。\nEnc_in： 8b/10b或者64b/6bb数据帧帧内编码错误。在正常情况下20分钟会出现一次这个报错，交换机端口（offline/online）会产生这个错误。\nCrc_err： 数据帧CRC校验错误。根据实际统计，如果crc_err和enc_out同时出现，通常代表GBIC/SFP有硬件问题。\nCrc_g_eof： 数据帧CRC校验错误，但是数据帧EOF是正常的。\nToo_long： 数据帧总长度超过2148字节或者workload长度超过2112字节。\nToo_short： 小于36个字节长度的帧（workload字节长度等于0）。\nBad_eof： 数据帧EOF错误。\nEnc_out： 8b/10b或者64b/66b数据帧帧外编码错误。在正常情况下20分钟会出现一次这个报错，交换机端口（offline/online）会产生这个报错，另外在HBA卡和交换机端口速率不同，而又使用的是静态配置端口速率的时候也会产生这个错误。单一的这个报错反映光纤线可能有问题；如果是Enc_out和crc_err同时报错代表GBIC/SFP有硬件问题。\nDisc c3： Class 3被交换机丢弃的数据帧。常见情形帧的目标地址不可达或者源端口还没有FLOGI交换机。这个参数仅仅代表有丢包发生，不能用来判定问题的具体原因。\nLink-fail： 当交换机端口在LR Receive State时间超过R_A_TOV就会产生这个错误。这个错误经常和loss of signal或者loss of sync同时出现。\nLoss sync： bit或者transmission-word synchronization失败都会产生这个错误。当交换机端口（offline/online）会产生这个问题。\nLoss sig： 链路收不到信号。当交换机端口（offline/online）会产生这个问题。\nFrjt： 用于class 2。代表数据帧无法处理。\nFrbsy： 用于class 2。数据帧无法在E_D_TOV时间内传输出去，超时后会产生这个问题。\n一般情况下：\n如果仅是\u0026quot;enc out \u0026ldquo;单独报错主要是因为光纤线的问题。\n如果是\u0026quot;enc out\u0026quot;和\u0026quot;crc err\u0026quot;组合报错主要是GBIC/SFP的问题。\n要确定是源端还是目标端SFP报错，需要再检查\u0026quot;portshow x\u0026rdquo; 的输出（x代表有问题端口号） 如果下面两对参数 \u0026ldquo;Lr_in \u0026quot; 和 \u0026ldquo;Ols_out \u0026quot; 以及 \u0026ldquo;Lr_out \u0026quot; 和\u0026quot;Ols_in \u0026quot; 的值相同，则表明SFP运行正常如果一个数值明显高于另一个, 连接问题可能出现在交换机连接的对端(\u0026ldquo;in\u0026rdquo; \u0026gt; \u0026ldquo;out\u0026rdquo;) 或是交换机本身(\u0026ldquo;out\u0026rdquo; \u0026gt; \u0026ldquo;in\u0026rdquo;).\n如果”Ols_in”的值高于“Lr_out”的值，问题的根源大多数情况与连接的设备相关，(sending those offline sequences) 并且交换机通过\u0026quot;link reset\u0026quot;对此做出响应。\nLoss sync，Loss sig，Link-fail这三个错误在链路初始化的过程中都会产生。当链路不稳定时候，通常这些错误计数器比较高。\nFrjt，Frbsy用于class 2。SAN存储通常使用的是class 3，所以这两个错误很少见。\nEnc_out和Crc_err两个计数器同时比较高，通常需要更换GBIC/SFP。\nDisk c3只能代表链路有丢包现象。原因可能有很多种，具体问题具体分析。如果这个值过高，链路性能可能会受到影响。\n","date":"2020-08-03T19:24:01Z","permalink":"https://example.com/post/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%8D%9A%E7%A7%91%E5%85%89%E4%BA%A4%E6%8E%92%E9%9A%9C/","title":"记一次博科光交排障"},{"content":"Ansible Tower的版本更新的也是莫名的快，去年安装的3.5今年已经到3.7.1了，新的版本可以在线申请license，旧的license快过期了，所以就升个级吧。\n步骤： 升级的时候不能只看升级文档，还要看新版本的support，一开始只看了升级文档，做的时候报操作系统版本不被支持，所以第一步配个较新的rhel源，然后update系统。\n1 2  # yum update -y # systemctl reboot    下载要升级的版本，并解压：\n1 2  # wget https://releases.ansible.com/ansible-tower/setup/ansible-tower-setup-3.7.1-1.tar.gz # tar xvzf ansible-tower-setup-3.7.1-1.tar.gz   把曾经3.5时候安装的inventory拷贝一份到3.7,注意要把老版本的rabbitmq_host有关的变量更改为routable_hostname，因为新版本移除了rabbitmq：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  # cp ansible-tower-bundle-3.5.1-1.el7/inventory ansible-tower-setup-3.7.1-1/ # cat inventory [tower] ansible01-ap ansible02-ap [database] [all:vars] admin_password=\u0026#39;redhat\u0026#39; pg_host=\u0026#39;10.xx.xx.xx\u0026#39; pg_port=\u0026#39;5432\u0026#39; pg_database=\u0026#39;towerdata\u0026#39; pg_username=\u0026#39;postgres\u0026#39; pg_password=\u0026#39;redhat\u0026#39; routable_username=\u0026#39;tower\u0026#39; routable_password=\u0026#39;redhat\u0026#39; routable_cookie=cookiemonster   因为涉及到clust所以用到了rhscl里的两个包，可以单独下下来建一个源，也可以直接陪rhscl的源，我enable了base和rhcsl两个源。 然后，在各节点上关闭服务：\n1  ansible tower -m command -a \u0026#34;ansible-tower-service stop\u0026#34; -i inventory   拆除主备实例关系：\n1 2 3 4 5 6 7 8  # awx-manage list_instances [tower capacity=154] ansible01-ap capacity=72 version=3.5.1 ansible02-ap capacity=72 version=3.5.1 # awx-manage deprovision_instance --hostname=ansible02-ap # awx-manage list_instances [tower capacity=0] ansible01-ap capacity=0 version=3.5.1   最后就可以运行setup了：\n1  # ./setup -i inventory   本来以为会很顺利，可是还是有点坑，不知道为什么ansible-tower的源通过proxy之后访问特别慢，而且还是总获取到ipv6的地址。所以找了一台可以连外网的机器，做了一个本地源：\n1 2 3  # yum repolist # yumdownloader --resolve --destdir /root/ansible ansible-tower # createrepo -v -g /root/ansible-tower   然而，这只是把tower相关的包下载下来做了一个本地镜像，实际运行setup的时候还是去找tower的源。 尝试用下面的命令同步整个源到本地，发现速度太慢了\n1  # reposync -l -p -m -g --download-metadata --repoid=ansible-tower --download_path=/root/ansible-tower   于是，翻了下playbook怎么写的，改了一下设置源的地方。有两个地方可以改，改一个就好了: 1、把源改成刚刚同步的最小的那个源的名：\n1 2 3  cat role/packages_el/defaults/main ansible_tower_repo: xxx ansible_tower_dependency_repo: xxx   2、把install_tower.yml里检查repo的部分去掉，要改两部分一个是ansible_tower_repo，一个是ansible_tower_dependency_repo，把相关这两个tasks注释掉就好了\n最后，等待setup完成就得到一个新版本的ansible tower 检查一下服务状态及实例情况，一切正常：\n1 2  # ansible-tower-service status # awx-manage list_instances   登录web，原来的project和inventory都在\n参考文档： https://docs.ansible.com/ansible-tower/3.7.1/html/installandreference/upgrade_tower.html#ir-upgrade-existing\n","date":"2020-07-27T17:39:01Z","permalink":"https://example.com/post/ansible-tower%E5%8D%87%E7%BA%A7/","title":"ansible tower升级"},{"content":"背景如题，方案在/etc/profile.d/下配置：\n1 2 3  # cat /etc/profile.d/http_proxy.sh export HTTP_PROXY=http://192.168.0.1:8080 export HTTPS_PROXY=http://192.168.0.1:8080   或直接export，但前者对ansible也生效，后者只能是该终端的用户\n貌似都是废话，其他程序配置Proxy，用常规的用export命令配置环境变量不生效的时候，也可以试试这样……\n","date":"2020-07-20T14:15:12Z","permalink":"https://example.com/post/set-up-proxy-for-podman/","title":"Set up proxy for podman"},{"content":"莫名其妙的曾经能用的mount cifs的方式开始报错\n1 2  mount error(112): Host is down Refer to the mount.cifs(8) manual page (e.g. man mount.cifs)   查了一下原因是smb协议版本问题，RHEL5和6最高支持SMB 1.0协议，RHEL7.2以后支持到SMB3。 所以，RHEL5和6就不要想挂载最新的windows共享了，windows目前关闭了smb1，RHEL7如果想访问windows共享的话，则需要指定一下版本。\n1  mount -t cifs -o vers=2.0,username=\u0026lt;win_share_user\u0026gt;,password=\u0026lt;win_share_password\u0026gt; //WIN_SHARE_IP/\u0026lt;share_name\u0026gt; /mnt/win_share   如果是域用户，则加domain信息：\n1  mount -t cifs -o vers=2.0,username=\u0026lt;win_share_user\u0026gt;,domain=\u0026lt;win_domain\u0026gt;，password=\u0026lt;win_share_password\u0026gt; //WIN_SHARE_IP/\u0026lt;share_name\u0026gt; /mnt/win_share   或者更安全一点的做法，建个文件储存用户名和密码：\n1 2 3 4 5 6 7 8 9 10 11  # cat /etc/win-credentials username = user password = password domain = domain 设置权限： chown root: /etc/win-credentials chmod 600 /etc/win-credentials 挂载： mount -t cifs -o credentials=/etc/win-credentials //WIN_SHARE_IP/\u0026lt;share_name\u0026gt; /mnt/win_share   也可以把信息写入/etc/fstab来实现自动挂载：\n1 2 3 4 5  # cat /etc/fstab # \u0026lt;file system\u0026gt; \u0026lt;dir\u0026gt; \u0026lt;type\u0026gt; \u0026lt;options\u0026gt; \u0026lt;dump\u0026gt; \u0026lt;pass\u0026gt; //WIN_SHARE_IP/share_name /mnt/win_share cifs credentials=/etc/win-credentials,file_mode=0755,dir_mode=0755 0 0 mount /mnt/win_share   ","date":"2020-07-13T13:50:07Z","permalink":"https://example.com/post/rhel%E6%8C%82%E8%BD%BDwindows%E5%85%B1%E4%BA%AB/","title":"RHEL挂载windows共享"},{"content":"Snap在ubuntu里简直是病毒一样的存在，一定要用purge才能删掉……\n顽固病毒也不过如此\n1  # apt autoremove --purge snapd   更彻底一点：\n1 2  # rm -rf /var/cache/snapd # rm -rf ~/snap   ","date":"2020-07-07T09:25:12Z","permalink":"https://example.com/post/ubuntu%E5%88%A0%E9%99%A4snap/","title":"Ubuntu删除snap"},{"content":"明天香港就真的归还给中共了，华人的净土没有了，虽然早知道会这样，却还是有点伤感……\n","date":"2020-06-30T15:26:10Z","permalink":"https://example.com/post/%E9%A6%99%E6%B8%AF/","title":"香港……"},{"content":"背景 终于准备用Nautilus做生产环境了！\n目前，Ceph 主要有三种企业级应用场景：\n IOPS 密集型：这种类型的场景通常是支撑在虚拟化/私有云上运行数据库。如在 OpenStack 上运行 Mysql、MariaDB 或 PostgreSQL 等。IOPS 密集型场景对磁盘的性能要求较高，最好使用全闪架构。如果使用混合架构，机械盘转速需要 1.2 万，并使用高速盘存储频繁写操作的日志或元数据。 高吞吐量型：这种类型的应用场景主要是大块数据传输，如图像、视频、音频文件等。高吞吐量型磁盘的要求没有 IOPS 密集型高，但需要配置较高的网络。同时也需要配置 SSD 来处理写日志。 高容量型：这种场景主要用于存储归档、离线数据。它对磁盘的容量要求高，对性能无过多要求。写日志也可以存储在 HDD 上。\n此次计划部署一套带宽型的用来提供S3服务，所以部署第二种就好。  硬件规划： nautilus采用blustore比filestore的性能大大提升，毕竟传输路径短了很多：\n随之而来的问题是BlueStore 存储引擎的实现，需要存储数据和元数据。目前 Ceph BuleStore 的元数据存储在 RocksDB（K-V 数据库）中。通过为 RocksEnv 提供操作接口，RocksDB 存放在 BlueFS 上。由于 BlueFS 最终通过 RocksDB，承载的是 BlueStore 存储引擎中的元数据，因此它的性能会很大程度上影响整个 Ceph 的性能。所以，要为它提供高速硬盘。\nRedhat建议，在新的bluestore架构下，如果用NVMe做metedata/jouranl，对应HDD的OSD是1：12；如果普通SSD做metedata/jouranl，则与HDD的比例为1：4 1 在此基础上，其他按照官方建议配置。    Device Size Count Use     CPU Inter Xeon 6252 2    Memor 32GB     DISK 4TB 12 OSD   SSD 480GB 2 Operation   NVMe 4TB 2 blockk.db   Network 10GB 2     以上配置保证了：\n1、每HDD的OSD至少对应5GB内存；\n2、每2 GHZ CPU每0.5 Cores每HDD；\n3、前端网络和后端网络的带宽，提供每12OSD有10GB带宽；\n4、SSD组成raid1保证操作系统安全；\n5、NVMe通过PCIE组成raid1，简化db磁盘损坏后的更换（此处不符合官方最佳实践）\n共6台这样硬件配置的服务器，每台的作用如下：    hostname IP addr Roles     rhcs01-st 10.64.0.80 10.64.4.80 OSD MONMGR   rhcs02-st 10.64.0.81 10.64.4.81 OSD RGWMDS   rhcs03-st 10.64.0.82 10.64.4.82 OSD MONMGR   rhcs04-st 10.64.0.83 10.64.4.83 OSD RGWMDS   rhcs05-st 10.64.0.84 10.64.4.84 OSD MONMGR   rhcs06-st 10.64.0.85 10.64.4.86 OSD RGWMDS    软件计划： 确定使用RHCS4.1来部署，其与开源版本对应关系如下：    Upstream Code Name Downstream Release Name Red Hat Ceph Storage Package Version (RHEL 8.x) Red Hat Ceph Storage Package Version (RHEL 7.x) Red Hat Ceph Ansible Package Version Release Month      Nautilus Red Hat Ceph Storage 4.1 14.2.8-59.el8cp 14.2.8-50.el7cp ceph-ansible-4.0.23-1 June, 2020    Nautilus Red Hat Ceph Storage 4.0 14.2.4-125.el8cp 14.2.4-51.el7cp ceph-ansible-4.0.14-1 January, 2020    而RHCS4支持的操作系统如下：    Vendor Version     Redhat Enterprise Linux 8.28.1   Redhat Enterprise Linux 7.87.7    因此，系统选用RHEL8.2\n小结 选定了软件版本后，按标准来配置即可。由于是分布式存储，主要还是考虑硬盘和网络，而硬盘的配置跟存储整个架构有很大关系。\n参考文档： https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/hardware_guide/index\n","date":"2020-05-20T15:16:05Z","permalink":"https://example.com/post/ceph%E8%A7%84%E5%88%92/","title":"CEPH规划"},{"content":"很多时候我们需要重复创建虚机，模板就是方便此类重复操作的一个虚机副本。模板不能拥有任何特定的信息，否则依此建立的虚机将都带有特定信息。所以，在建立模板前需要对源虚机进行封装，所谓封装就是在保证其他功能正常的同时，去除掉特点的功能。\n以前linux通用的封装方式比较烦，需要手动删除如下：\n ssh主机密钥：  1  rm -rf /etc/ssh/ssh_host_*   设置hostname为localhost  1 2  cat /etc/sysconfig/network HOSTNAME=localhost.localdomain   从/var/log中删除所有日志：  1  for i in `find .-name \u0026#34;*.log\u0026#34;`;do cat /dev/null\u0026gt;$i;done   /root中删除build日志： 关闭虚机  现在rhel提供了更简便的封装方法：\n1 2  yum install -y libguestfs-tools virt-sysprep   windows的封装一向很简单：\n运行sysprep.exe\n一般在C:\\Windows\\System32\\sysprep\\sysprep.exe\n此类是通用的封装方法，有虚拟化平台提供自己的封装工具，那么以平台为准。\n","date":"2020-05-12T11:19:16Z","permalink":"https://example.com/post/%E5%B0%81%E8%A3%85%E8%99%9A%E6%8B%9F%E6%9C%BA/","title":"封装虚拟机"},{"content":"总会遇到诡异的问题，比如要部署厂家打包好的定制化的镜像，发现这镜像只提供一种平台的格式，这时候就需要将虚拟磁盘做转换了。\n各个厂家提供一些定制化的工具并不是太通用，比如VMWare的vmware-vdiskmanager，好在这个世界有开源的qemu-img工具，可以方便的转换各种格式的镜像。\n安装： qemu-img提供多平台支持，常见的windows可以从官网下载安装；linux就更简单了，配源然后install，比如：yum install qemu-img\n参数： 常用的转换参数如下：\nconvert：转换操作；\n-p：显示转换进度;\n-f xxx：指明被转换镜像的格式，可以留空，但hyperV的vhd最好写明vpc否则可能不识别；\n-O xxx：表明要转换成的格式；\ninfo：查询信息\n实例： vmdk转qcow2：\n1  qemu-img convert -p -f vmdk -O qcow2 RHEL.vmdk RHEL.qcow2   查询镜像详细信息：\n1  qemu-img info RHEL.qcow2   如果转vhd文件，注意-f后跟vpc而不是vhd\n以上没有了……\n","date":"2020-05-08T11:00:23Z","permalink":"https://example.com/post/%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F%E8%BD%AC%E6%8D%A2/","title":"虚拟机镜像转换"},{"content":"一直不知道为啥红帽会把RHHI放进存储产品线，明明就是个虚拟化环境的超融合环境，甚至安装介质跟rhv都是一样的，两者只有部署方式上的差异。\n从最开始的RHV一直用到现在的RHHI，红帽的产品没有VMWare那么丰富的接口，却也别有特色，用习惯了感觉也不错。才不是用多了国产的烂玩意才有的这种感觉。\n这两天环境被弄乱了，就图省事不修了，重新部署了一套了事，记录一点过程：\n首先，要有RHVH的订阅，没有就去下ovirt，rhv对应的开源版本就是它；\n其次，红帽对此的文档已经很详细了，也不需要帐号就可以阅读，相关的两篇文档如下：\nhttps://access.redhat.com/documentation/en-us/red_hat_hyperconverged_infrastructure_for_virtualization/1.7/html/deploying_red_hat_hyperconverged_infrastructure_for_virtualization/deployment_workflow#enabling-software-repositories\nhttps://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4-beta/html/administration_guide/sect-preparing_and_adding_nfs_storage\n注意，第一篇部署文档里没提到的是宿主机的主机名需要被DNS解析，否则部署host-engin的时候会过不去地址检测。\n第二篇文档要注意的是nfs的权限设置，不是随便哪个nfs都能在rhv环境中挂起来的。\n然后，按照文档部署完成之后要在compute-\u0026gt;cluster里把enable gluster service的勾勾上，不然在storage-\u0026gt;volumes里管理不到先前创建的gluster卷。\n最后，就可以慢慢用了，从rhv2到3再到如今的4.3，改变还是蛮大的，也明显越来越好用了。虽然不是主流产品，但是如果有超融合需求，不妨尝试一下，毕竟有免费的开源版本ovirt。\n但是4.3的RHV有个最坑的地方，由于宿主机是基于RHEL7的系统，因此无法将RHEL8的系统做成模板，因为rhel8中xfs有新特性，而这点居然写在了RHV4.4的文档里，而不是4.3的……\n","date":"2020-05-05T13:57:26Z","permalink":"https://example.com/post/%E9%83%A8%E7%BD%B2rhhi%E7%8E%AF%E5%A2%83/","title":"部署RHHI环境"},{"content":"做磁带转储的时候发生了一件很奇怪的事情，一盘LTO3的磁带被认成了LTO2，在LTO5的驱动器里读不出来。\n当然读不出来很正常，毕竟只向下兼容两代读，可认成LTO2就很诡异了。\n于是找了一下方法把它修改过来：\n1、CV管理员登录：\ncd到安装目录然后qlogin，按提示输入用户名和密码；\n2、对要操作的介质执行：\nQOperation.exe execscript -sn changeMediaType -si media -si \u0026ldquo;000231\u0026rdquo; -si \u0026ldquo;ULTRIUM V3\u0026rdquo;\n","date":"2020-04-27T16:53:56Z","permalink":"https://example.com/post/commvault%E4%BF%AE%E6%94%B9%E4%BB%8B%E8%B4%A8%E4%BF%A1%E6%81%AF/","title":"Commvault修改介质信息"},{"content":"使用LVM的好处之一就是可以用pvmove来方便的在线迁移数据。用pvmove来迁移、合并数据比创建mirorr的方式要安全而方便，它会把要迁移的数据拆分成多分，并通过建立临时镜像的方式来移动数据，即使中断也有全部的数据，还能很方便的再次开始。\n简单的用法： 1  pvmove /dev/sdc1   将/dev/sdc1上的数据迁移到vg中的其他物理卷\n1  pvmove -n MYLV /dev/sdc1   将MYLV迁移到物理卷/dev/sdc1\n1  pvmove -b /dev/sdc1 /dev/sdd1   pvmove的最常用用法，在后台迁移/dev/sdc1的数据到/dev/sdd1\n1  pvmove -i5 /dev/sdc1   查询pvmove的进度，每5s输出一次结果。可以随时执行以查询迁移进度，后面不指定迁移的磁盘则会显示所有pvmove的进度\n1  lvs -a -o +devices   另一种查询迁移进度的方式\n迁移完成后： 1 2  vgreduce /dev/sda pvremove /dev/sda   迁移完成后即可将不用的物理卷从vg中剔除，把它的PV属性去掉\n注意： 虽然可以在线迁移，但仍会增大CPU、MEMORY、IO的开销，所以尽量在不忙的时候做操作。\n如果要迁移clvmd管理的集群LVM，则需要确保cmirrord服务安装并开启。\n","date":"2020-04-12T15:29:48Z","permalink":"https://example.com/post/pvmove%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE/","title":"pvmove迁移数据"},{"content":"Caddy有个模块很好用，经过简单的配置即可以https或http2的方式跨越防火墙，不需要单独申请证书同时还有正常网站做伪装，配合浏览器switch omega插件，方便快捷的访问被墙的资源。\n安装： 1  curl https://getcaddy.com | bash -s personal http.forwardproxy   配置一个正常网站： 创建文件并赋权等杂项：\n1 2 3 4 5 6 7 8 9 10 11 12 13  setcap \u0026#39;cap_net_bind_service=+ep\u0026#39; /usr/local/bin/caddy useradd -r -d /var/www -M -s /sbin/nologin caddy chown -R caddy:caddy /var/www mkdir /etc/caddy chown -R root:caddy /etc/caddy touch /etc/caddy/Caddyfile chown caddy:caddy /etc/caddy/Caddyfile mkdir /var/log/caddy chown caddy:caddy /var/log/caddy/ mkdir -p /var/www/xxx.yyy.zzz chown -R caddy:caddy /var/www mkdir -p /etc/ssl/caddy chown -R caddy:caddy /etc/ssl/caddy   编辑配置文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  vim /etc/caddy/Caddyfile https://xxx.yyy.zzz { //xxx.yyy.zzz为可以解析到本机的域名，下同  log /var/log/caddy/xxx.log errors /var/log/caddy/xxx.error root /var/www/xxx.yyy.zzz gzip tls \u0026lt;xxx@xxxxx.xxx\u0026gt; { //随便一个邮箱  ciphers ECDHE-ECDSA-WITH-CHACHA20-POLY1305 ECDHE-ECDSA-AES256-GCM-SHA384 ECDHE-ECDSA-AES256-CBC-SHA curves p384 key_type p384 } header / { Strict-Transport-Security \u0026#34;max-age=31536000;\u0026#34; X-XSS-Protection \u0026#34;1; mode=block\u0026#34; X-Content-Type-Options \u0026#34;nosniff\u0026#34; X-Frame-Options \u0026#34;DENY\u0026#34; } }   编辑一个简单的网页：\n1 2 3 4 5 6 7 8 9 10 11  vim /var/www/xxx.yyy.zzz/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Hello from Caddy!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1 style=\u0026#34;font-family: sans-serif\u0026#34;\u0026gt;This page is being served via Caddy!!!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   测试是否可以正常建立网站：\n1  caddy -conf /etc/caddy/Caddyfile   按提示填写信息生成证书，如果没有报错，至此，应该可以打开一个网页，内容是加黑的This page is being served via Caddy!!!。\n配置代理： 在caddy的配置文件中加入forwardproxy信息，加入后内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  https://xxx.yyy.zzz { //xxx.yyy.zzz为可以解析到本机的域名，下同  log /var/log/caddy/xxx.log errors /var/log/caddy/xxx.error root /var/www/xxx.yyy.zzz gzip tls \u0026lt;xxx@xxxxx.xxx\u0026gt; { //随便一个邮箱  ciphers ECDHE-ECDSA-WITH-CHACHA20-POLY1305 ECDHE-ECDSA-AES256-GCM-SHA384 ECDHE-ECDSA-AES256-CBC-SHA curves p384 key_type p384 } forwardproxy { basicauth xxx yyy //xxx yyy为用户及密码 \thide_ip hide_via probe_resistance caddyserver.com } header / { Strict-Transport-Security \u0026#34;max-age=31536000;\u0026#34; X-XSS-Protection \u0026#34;1; mode=block\u0026#34; X-Content-Type-Options \u0026#34;nosniff\u0026#34; X-Frame-Options \u0026#34;DENY\u0026#34; } }   其中probe_resistance可以隐藏代理类型，是个实验功能，可加可不加\n证书： caddy默认生成的证书在/root/.caddy下，ln它到/etc/ssl/caddy\n1 2  ln /root/.caddy/acme/acme-v02.api.letsencrypt.org/sites/xxx.yyy.zzz/xxx.yyy.zzz.key /etc/ssl/caddy/ ln /root/.caddy/acme/acme-v02.api.letsencrypt.org/sites/xxx.yyy.zzz/xxx.yyy.zzz.crt /etc/ssl/caddy/   日志权限： 1  chown caddy:caddy /var/log/caddy/*   管理服务： 配置systemd管理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  vim /etc/systemd/system/caddy.service [Unit] Description=Caddy HTTP/2 web server Documentation=https://caddyserver.com/docs After=network-online.target Wants=network-online.target systemd-networkd-wait-online.service [Service] Restart=on-abnormal ; User and group the process will run as. User=caddy Group=caddy ; Letsencrypt-issued certificates will be written to this directory. Environment=CADDYPATH=/etc/ssl/caddy ; Always set \u0026#34;-root\u0026#34; to something safe in case it gets forgotten in the Caddyfile. ExecStart=/usr/local/bin/caddy -log stdout -agree=true -conf=/etc/caddy/Caddyfile -root=/var/tmp ExecReload=/bin/kill -USR1 $MAINPID ; Use graceful shutdown with a reasonable timeout KillMode=mixed KillSignal=SIGQUIT TimeoutStopSec=5s ; Limit the number of file descriptors; see `man systemd.exec` for more limit settings. LimitNOFILE=1048576 ; Unmodified caddy is not expected to use more than that. LimitNPROC=512 ; Use private /tmp and /var/tmp, which are discarded after caddy stops. PrivateTmp=true ; Use a minimal /dev PrivateDevices=true ; Hide /home, /root, and /run/user. Nobody will steal your SSH-keys. ProtectHome=true ; Make /usr, /boot, /etc and possibly some more folders read-only. ProtectSystem=full ; … except /etc/ssl/caddy, because we want Letsencrypt-certificates there. ; This merely retains r/w access rights, it does not add any new. Must still be writable on the host! ReadWriteDirectories=/etc/ssl/caddy ; The following additional security directives only work with systemd v229 or later. ; They further retrict privileges that can be gained by caddy. Uncomment if you like. ; Note that you may have to add capabilities required by any plugins in use. CapabilityBoundingSet=CAP_NET_BIND_SERVICE AmbientCapabilities=CAP_NET_BIND_SERVICE NoNewPrivileges=true [Install] WantedBy=multi-user.target   至此，可以通过systemd统一管理caddy服务：\n1  systemctl status caddy.service -l   配置客户端： PC端firefox及chrome可以安装Switch Omega，然后添加一个https代理即可，注意添加用户名和密码否则每次都要输 IOS端可以shadowrocket，配置http2代理和https代理均可。\n然后，然后就没有然后了……\n","date":"2020-04-06T10:09:28Z","permalink":"https://example.com/post/caddy%E7%BF%BB%E5%A2%99/","title":"Caddy翻墙"},{"content":"内网环境往往需要改pip为内网源地址，否则要配Proxy。\n临时更改： 1  pip install -i \u0026lt;server IP\u0026gt; --trusted-host \u0026lt;server IP\u0026gt; \u0026lt;package_name\u0026gt;   永久修改： linux修改 ~/.pip/pip.conf中的index-url和trusted-host，windows修改C:\\Users\\xxx\\pip\\pip.ini (没有就创建一个)\n1 2 3 4 5  [global] index-url = pypi.python.org trusted-host = pypi.python.org pypi.org files.pythonhosted.org   ","date":"2020-03-25T10:08:05Z","permalink":"https://example.com/post/python-%E6%9B%B4%E6%94%B9pip%E6%BA%90/","title":"Python 更改pip源"},{"content":"Vim可以在命令模式下使用substitute命令，将指定的字符串替换成目标字符串。\n基本语法： Vim替换命令的基本语法是 :[range]s/目标字符串/替换字符串/[option]，其中range和option字段都可以缺省不填。\nrange: 表示搜索范围，默认表示当前行; 也可以是以逗号隔开的两个数字，例如1,20表示从第1到第20行; 也可以是特殊字符，与其他地方的默认习俗一致，其中%表示整个文件\ns 即substitute的简写，表示执行替换字符串操作;\noption: 表示操作类型，默认只对第一个匹配的字符进行替换； 常见字段值有：g(global)表示全局替换; c(comfirm)表示操作时需要确认; i(ignorecase)表示不区分大小写;\n简单实例 替换当前行第一个 VIM 为 sakuya :s/VIM/sakuya/\n替换当前行所有 VIM 为 sakuya :s/VIM/sakuya/g \n替换第 n 行开始到最后一行中每一行的第一个 VIM 为 sakuya :n,$s/VIM/sakuya/ \n替换第 n 行开始到最后一行中每一行所有 VIM 为 sakuya\n:n,$s/VIM/sakuya/g  替换每一行的第一个 VIM 为 sakuya\n:%s/VIM/sakuya/ 等同于： :g/VIM/s//sakuya/\n替换每一行中所有 VIM 为 sakuya :%s/VIM/sakuya/g 等同于： :g/VIM/s//sakuya/g\n可以使用 # 作为分隔符，此时中间出现的 / 不会作为分隔符，例如：替换当前行第一个 VIM/ 为 sakuya/ :s#VIM/#sakuya/# \n全文的行首加入//字符，批量注释时非常有用，/进行转义后才可以替换 :%s/^/\\/\\//\n将所有行尾多余的空格删除 :%s= *$==\n","date":"2020-03-20T10:05:31Z","permalink":"https://example.com/post/vim%E6%9B%BF%E6%8D%A2/","title":"Vim替换"},{"content":"遇到个包xx.tar.xz打包的，感觉这格式很罕见，有点懵，所以查了一下要怎么解压，顺便把tar复习一下。\ntar.xz解压： 1 2 3  # yum install -y xz # xz -d ***.tar.xz # tar -xvf ***.tar   这种格式本质是打包了两层\n也可以不装xz，直接使用 tar xvJf **.tar.xz来解压\nTar选项： 1 2 3 4 5 6 7 8 9  c – 创建压缩文件 x – 解压文件 v – 显示进度. f – 文件名. t – 查看压缩文件内容. j – 通过bzip2归档 z –通过gzip归档 r – 在压缩文件中追加文件或目录 W – 验证压缩文件   ","date":"2020-03-16T11:08:56Z","permalink":"https://example.com/post/tar.xz%E8%A7%A3%E5%8E%8B/","title":"tar.xz解压"},{"content":" author: \u0026ldquo;Suika\u0026rdquo; title: \u0026ldquo;解决UWP应用无法联网的三种方法\u0026rdquo; date: \u0026ldquo;2020-03-12 14:07:17\u0026rdquo; description: \u0026ldquo;怎么会有UWP这种东西\u0026rdquo; categories: \u0026ldquo;Tech\u0026rdquo; tags:\n \u0026ldquo;UWP\u0026rdquo; “Proxy” image: \u0026quot;\u0026quot;   surface Pro 6又坏了，微软的硬件好垃圾，一年多坏了3次，再不会买了，影响工作效率。每次送修的结果就是所有东西都要重配一遍，这次是换机，更彻底一点。然后，又遇到了当初第一次用surface的时候遇到的问题——UWP应用联网。于是把备份的一篇文字直接拿来解决这个问题，再发一遍方便以后找。\n 从Arch到Win10各种不适应，最近这种不适应在一个始料未及的事情上表现的尤为明显——只要开了全局代理，微软自家的UWP程序就无法联网了。\n对此，在下也是服气。\n然而，发现问题不解决不是吾辈的作风，于是开始找各种解决方案，顺带了解了一下UWP。\n这玩意出来也好几年了，一直不温不火的，以至于我从来没有注意到还有这么个玩意。据说所有UWP应用均运行在被称为App Container的虚拟沙箱环境中，其安全性胜于传统的EXE应用。但问题也出在这里，安全的同时它阻止了网络流量发送到本机（即loopback），即阻止了UWP应用访问 localhost。没有访问回环地址的权限，也意味着即便系统设置中启用了本地代理，UWP应用也无法访问本地代理服务器。\n按照官方的解决方案，貌似可以用CheckNetIsolation命令来解决这个问题，MSDN轻描淡写的写着命令：\n1 2 3  checknetisolation loopbackexempt -a -n= UWP应用容器名 checknetisolation loopbackexempt -a -p= UWP应用SID   可是谁告诉我UWP的SID去哪里找？UWP的应用容器名称又是啥？\n有一种找SID的方法，通过注册表获取：\nwin+r的运行里输入Regedit打开注册表编辑器，然后定位到\n1 2 3 4 5 6 7 8  HKEY_CURRENT_USERSoftwareClassesLocal Settings Software MicrosoftWindows CurrentVersion App Container Mappings   接着在左边的注册表项中找到你想解除网络隔离的应用，右边的 DisplayName 就是应用名称，而左边那一大串字符就是应用的SID值了。\nSID值是人类可以理解的么？对此我可以骂人么？这是人干的事？我不能骂人么？那我还有什么话好说。\n这方法还是放弃的好！\n官方的解决方案如此复杂，按照问题发生后最简单的思路去解决，那就是关闭代理。\n这简单粗暴的方法确实可行，唯一的问题是关闭代理这事在国内是我不能接受的。\n接着搜索，互联网给出的答案是网络调试工具Fiddler，然后左上角的WinConfig按钮，排除不要的UWP应用。\n可是，Fiddler这工具用来干这个岂不是太大材小用了？\n最终，我找到了这个——Windows-Loopback-Exemption-Manager\nGithub上的项目，看来遭这问题困扰的人好多啊\nhttps://github.com/tiagonmas/Windows-Loopback-Exemption-Manager\n好了，这玩意跟Fiddler用法差不多，功能更单一，仅此而已。\n哦，这东西还有一个坑，就是邮件，用这种方法邮件还是没办法添加Gmail不知道是为啥。\n以上就是三种解决方案，其实只能算两种:\n如果想简单粗暴，那么关闭代理是最直接的,这简直不能算解决方案；\n不然，就Fiddle或是Windows-Loopback-Exemption-Manager这两者虽然有坑都还可以接受；\n我想没有人愿意手动去一条条命令敲吧，光是找SID的工作就是非人类的。\n","date":"2020-03-12T14:07:17Z","permalink":"https://example.com/post/%E8%A7%A3%E5%86%B3uwp%E5%BA%94%E7%94%A8%E6%97%A0%E6%B3%95%E8%81%94%E7%BD%91%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95/","title":"解决UWP应用无法联网的三种方法"},{"content":"主力的本子用ARCH工作已经几年了，一直没有问题，有问题也都是能解决的问题。所以是出于莫名其妙的原因吧，在这个假期把arch重装了一下。跟几年前有些许变化，趁还有日志，记录一下过程。\n一、环境准备： 设置bios为UEFI启动，各主机情况不同，一般是开机按F2进主板进行设置，设置的同时在boot里将从其他设备启动的选项打开；\n现在没有本子不是UEFI启动了吧，如果有就去看官方文档吧，这篇不适合做参考。\n下载最新的arch linux的ISO镜像，然后制作启动U盘。\n我是在现有的arch上用dd做的：\n1  dd if=archlinux.iso of=/dev/sdb bs=1M   然后，用U盘引导启动，进入live CD的系统后再确认一下启动方式是什么：\n1  # ls /sys/firmware/efi/efivars   如果此目录下有文件则为UEFI，否则请重新检查bios设置。\n国内的主机默认美国键盘就好，不需要做任何更改。\n二、连接到网络： 有线默认是dhcp的，插上网线自动获取地址即可，无线稍微复杂一点，可选的工具也很多，我习惯用netctl这工具。\n检查网卡名称：\n1  ip link   拷贝并修改配置文件：\n1 2  # cp /etc/netctl/examples/wireless-wpa /etc/netctl/ihangzhou # vim /etc/netctl/ihangzhou   修改了以下标记了的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13  Description=\u0026#39;A simple WPA encrypted wireless connection\u0026#39; Interface=~~wlp2s0~~ Connection=wireless Security=wpa IP=dhcp ESSID=\u0026#39;~~i-hangzhou~~\u0026#39; Key=\u0026#39;~~xxxxxx~~\u0026#39; wifi的密码 # Uncomment this if your ssid is hidden Hidden=*~~yes~~* 是否是隐藏的网络，如不是则在此行前面加#将其注释掉，默认注就是注释的 # Set a priority for automatic profile selection Priority=10   启用配置：\n1 2  cd /etc/netctl/ netctl start ihangzhou   如果没有报错应该就会获取到地址，有报错就检查一下哪里错了。\n确认一下地址，并看下联通性，这时候百度有了它唯一的用处：\n1 2  # ip a # ping www.baidu.com   三、磁盘分区： 磁盘分区和文件系统都算常识吧，有兴趣的去看wiki或google，此次只是记录下简单的使用GPT分区和brtfs的过程。\n1 2 3 4 5 6  # parted /dev/sdx (parted) mklabel gpt (parted) mkpart ESP fat32 1M 150M (parted) set 1 boot on (parted) mkpart primary brtfs 151M 15G (parted) mkpart primary brtfs 24.5G 100%   将分区格式化：\n1 2 3  # lsblk /dev/sdx # mkfs.vfat -F32 /dev/sdxY # mkfs.brtfs /dev/sdxZ   创立挂载点并挂载分区：\n1 2 3 4 5 6  # mount /dev/sdxZ /mnt 根分区要先挂载，然后再创建其他挂载点 # mkdir /mnt/home # mount /dev/sdxW /mnt/home # mkdir -p /mnt/boot # mount /dev/sdXY /mnt/boot   我没有创建swap分区，我觉得内存足够大够用了。\n四、安装系统： 选用合适的安装镜像地址\n要根据网络情况来调整，列表中排的越靠前的镜像站优先级越高，同时理论地理位置越近的镜像站速度越快，国内墙是主要考虑因素，按实际情况调整即可，以获得尽量大的下载速度为主要目标。\n更改镜像列表后请务必强制刷新。\n1 2  # vim /etc/pacman.d/mirrorlist # pacman -Syy   安装基本软件包\n1  # pacstrap /mnt base linux linux-firmware   此处也可以同时安装其他的软件包，把包名加在后面即可\n生成 fstab：\n最好用-U参数，使用设备UUID而不是别名进行挂载\n1  # genfstab -U -p /mnt \u0026gt;\u0026gt; /mnt/etc/fstab   五、配置环境： 进入安装好的系统：\n1  # arch-chroot /mnt   调整时间：\n开启NTP同步：\n1  # timedatectl set-ntp true   选择时区：\n1 2  # tzselect # ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime   同步硬件时间：\n1  # hwclock --systohc   本地化设置：\nlocale.gen 与 locale.conf两个文件，规定地域、货币、时区日期的格式、字符排列方式和其他本地化标准等等，非常重要。\n/etc/locale.gen是一个仅包含注释文档的文本文件。指定需要的本地化类型，只需移除对应行前面的注释符号（＃）即可，建议选择带UTF-8的选项，之后再生成locale信息：\n1 2 3 4 5 6 7  # vim /etc/locale.gen en_US.UTF-8 UTF-8 zh_CN.UTF-8 UTF-8 zh_TW.UTF-8 UTF-8 ja_JP.UTF-8 UTF-8 # locale-gen   创建 locale.conf 并提本地化选项：\n1  # echo LANG=en_US.UTF-8 \u0026gt; /etc/locale.conf   创建初始ramdisk环境:\n打开需要的钩子，比如lvm什么的，然后生成Initramfs\n1 2  # vim /etc/mkinitcpio.conf # mkinitcpio -p linux   设置root密码：\n1  # passwd   安装启动器：\n再次确认一下分区挂载情况，尤其是boot分区，不正确还可以调整：\n1  # lsblk   启动器有很多可选，grub2太传统，这次选择了syslinux，如果要好看可以选rEFInd。\n我才不会说是搞了半天搞不定rEFInd才换syslinux的，逃\n1 2  # pacman -S syslinux # syslinux-install_update -i -a -m   安装其他包：\n1  # pacman -S networkmanager wpa_supplicant dialog bash-completion dhcpd   重启进入系统：\n1 2  # exit # reboot   其他设置：\n联网：\n1  # nmtui   设置主机名：\n1  # hostnamectl set-hostname XXX   添加管理员帐号：\n1 2  # useradd -m -g users -G wheel -s /bin/bash xxx # passwd xxx   更新一下系统：\n1  # pacman -Syyu   然后就完成了，后面图形什么的是另外的事。\n我用了xfce4做图形桌面，fcitx做输入法，挺好用的。\n六、其他工具： 如今arch的安装其实很简单，手操一遍有助于对linux工作方式和启动的理解。如果不想手动的操作，github上有专门的辅助部署的项目，比较完善的有helmuthdu/aui连后面的图形和软件一起都搞定了，只有你能进入live CD环境并联网。\n https://wiki.archlinux.org/index.php/Installation_guide\nhttps://wiki.archlinux.org/index.php/General_recommendations\nhttps://wiki.archlinux.org/index.php/List_of_applications\nhttps://telegra.ph/%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85ARCH-LINUX-07-18\n ","date":"2020-03-08T17:48:42Z","permalink":"https://example.com/post/arch-linux-instal/","title":"Arch Linux instal"},{"content":"CEPH可以通过mgr节点的zabbix模块实现监控\n不同于普通的agent模式，CEPH自带的zabbix模块通过trapper模式工作，mgr节点收集集群信息并向Zabbix Server发送。\n1、环境说明： 3节点CEPH集群，每个node都是mgr节点;\n部署Nautilus 14.2.7；\nZabbix Server版本：4.2.1\n2、安装部署： 在所有mgr节点上安装zabbix-sender\n配置yum：\n1  # rpm -Uvh https://repo.zabbix.com/zabbix/4.4/rhel/7/x86_64/zabbix-release-4.4-1.el7.noarch.rpm   安装sender：\n1  # yum install zabbix-sender -y   3、配置zabbix模块： 在任意一个mgr节点上执行如下配置：\n启用zabbix module：\n1  # ceph mgr module enable zabbix   指定zabbix server，如果有多个可以用\u0026quot;,\u0026ldquo;隔开，并且可以添加\u0026rdquo;:\u0026ldquo;来指定服务器端口，此处只有最简单的配置：\n1  # ceph zabbix config-set zabbix_host zabbix.test.com   指定zabbix_sender的位置：\n1 2 3 4 5 6  # which zabbix_sender /usr/bin/zabbix_sender # ceph zabbix config-set zabbix_sender /usr/bin/zabbix_sender Configuration option zabbix_sender updated   指定identifier，这里很奇怪，只能是某个mgr节点的主机名，其他的可以设置成功但是发送失败，不知道我哪里配的有问题：\n1  # ceph zabbix config-set identifier \u0026#34;ceph01.test.com\u0026#34;   后面如果有需要可以定义server的端口等，命令跟前面的相仿，这里就不配了。\n查看一下当前的配置：\n1  # ceph zabbix config-show   查看一下自带的监控模板，后面会用到：\n1 2  # find / -iname zabbix_template.xml /usr/share/ceph/mgr/zabbix/zabbix_template.xml   4、配置zabbix server： 登录Zabbix Web UI导入前面找到的监控模板，模板也可以在github中找，默认选项导入即可：\n创建主机组：\n添加主机并关联主机组和模板，如果有需要主机可以关联多个模板，比如系统的监控：\n配置用户告警： 至此配置完成。\n5、测试配置 任意一台mgr节点测试：\n1 2  # ceph zabbix send Sending data to Zabbix   如果发送不成功会导致ceph健康状态告警，检查端口通信情况及zabbix server的trapper模式是否被禁用，以及上面的步骤是不是哪里配置错了。\n https://docs.ceph.com/docs/master/mgr/zabbix/\nhttps://www.cnblogs.com/lbjstill/p/12169820.html\n ","date":"2020-03-02T15:13:58Z","permalink":"https://example.com/post/ceph-nautilus-zabbix-module-setup/","title":"CEPH Nautilus Zabbix module Setup"},{"content":"背景： RHEL7的日志（/var/log/message）里总有如下log，甚至很长一段时间内整个log都是这些内容：\n1 2 3 4 5 6  example.com systemd: Created slice user-0.slice. example.com systemd: Starting Session 150 of user root. example.com systemd: Started Session 150 of user root. example.com systemd: Created slice user-0.slice. example.com systemd: Starting Session 151 of user root. example.com systemd: Started Session 151 of user root.   过滤： 照红帽的官方说法，这是居然是正常情况，只要用户登录就会记录这些日志，如果不想记录这些内容需要主动将其过滤掉，方法如下：\n1  echo \u0026#39;if $programname == \u0026#34;systemd\u0026#34; and ($msg contains \u0026#34;Starting Session\u0026#34; or $msg contains \u0026#34;Started Session\u0026#34; or $msg contains \u0026#34;Created slice\u0026#34; or $msg contains \u0026#34;Starting user-\u0026#34; or $msg contains \u0026#34;Starting User Slice of\u0026#34; or $msg contains \u0026#34;Removed session\u0026#34; or $msg contains \u0026#34;Removed slice User Slice of\u0026#34; or $msg contains \u0026#34;Stopping User Slice of\u0026#34;) then stop\u0026#39; \u0026gt;/etc/rsyslog.d/ignore-systemd-session-slice.conf   然后，重启rsyslog服务：\n1  systemctl restart rsyslog   其他： 以上方法只对本地日志起作用，远程日志系统不适用。 如果远程日志要过滤此类日志，需要把上述规则写进/etc/rsyslog.conf，此时要写在发送到日志服务器之前，即. @@sys-log server这一行前。\n","date":"2020-02-26T15:40:16Z","permalink":"https://example.com/post/created-slice-ampamp-starting-session/","title":"Created slice \u0026amp;amp; Starting Session"},{"content":"新开端 居家办公可以算是好长的一个假期，自工作以来再没想过能有这样长的一个假。\n不能出门的日子有人会觉得无聊，可是我不会。上班、健身、做饭、学习、炒汇、阅读、补电影，被宅在家里的日子，每一天都感觉不够用。终于有大块的时间做自己想做的事情了，多幸福！\n从这个意义上或许应该要感谢一下这次瘟疫？\n在某个阳光明媚的午后突然就觉得应该记录一下这样难得的日子，继而就想起前年搭的那个环境。那是第一台VPS，为了榨取它的价值，不仅被我用来搭梯子，也被我用作记录生活和想法。陆陆续续写了大半年，后来到期了就没有续费，临终的时候在Telegraph做了个备份，算是给自己留给回忆。如今又有了记录的想法，为了避免上一次荒废的悲剧，我决定依赖Github来建静态站，这样可以最大限度的避免外因导致的前功尽弃。\n于是，现在这个界面出现了。我将在此记录日常的想法、生活的点滴以及技术的学习，写下自己的文字，记录生命中的点滴。\n唯愿此事能长久！\n 延续 不知不觉疫情已经第三个年头了，还是看不到尽头。好在偶尔还能想起这个记录的地方，还没有荒废。\n可是，回头看看快做成技术笔记了，跟初衷背离挺远的。我更希望能有个地方长篇的记录想法，毕竟零星的短篇都记在Telegram Channel了。\n所以，决定换个风格！\n而且，Gridea已经用了两年多了，想换个能随时编辑的，因此就有了这次折腾，换到了HUGO，持续继承什么的最棒了。\nAnyway，希望记录想法这事能持续！\n","date":"2020-02-20T09:32:34Z","permalink":"https://example.com/post/%E5%86%99%E5%9C%A8%E5%BC%80%E5%A7%8B%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5/","title":"写在开始的碎碎念"}]